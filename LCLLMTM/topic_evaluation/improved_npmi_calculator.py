#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NPMIè¿è´¯æ€§è®¡ç®—å™¨
"""

import re
import math
import numpy as np
from typing import List, Dict, Tuple, Optional, Set
from collections import defaultdict, Counter
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import warnings
warnings.filterwarnings('ignore')

# ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')

class ImprovedNPMICalculator:
    """æ”¹è¿›çš„NPMIè¿è´¯æ€§è®¡ç®—å™¨"""
    
    def __init__(self, reference_corpus: List[str], window_size: int = 10):
        """
        åˆå§‹åŒ–NPMIè®¡ç®—å™¨
        
        Args:
            reference_corpus: å‚è€ƒè¯­æ–™åº“
            window_size: æ»‘åŠ¨çª—å£å¤§å°
        """
        self.reference_corpus = reference_corpus
        self.window_size = window_size
        self.lemmatizer = WordNetLemmatizer()
        
        # æ‰©å±•åœç”¨è¯åˆ—è¡¨
        self.stop_words = set(stopwords.words('english'))
        self.stop_words.update([
            'said', 'say', 'says', 'saying', 'would', 'could', 'should',
            'one', 'two', 'three', 'first', 'second', 'last', 'next',
            'new', 'old', 'good', 'bad', 'big', 'small', 'high', 'low',
            'many', 'much', 'more', 'most', 'some', 'any', 'all', 'each',
            'every', 'other', 'another', 'same', 'different', 'such',
            'way', 'ways', 'time', 'times', 'year', 'years', 'day', 'days',
            'week', 'weeks', 'month', 'months', 'hour', 'hours',
            'make', 'made', 'take', 'taken', 'get', 'got', 'give', 'given',
            'go', 'went', 'come', 'came', 'see', 'seen', 'know', 'known',
            'think', 'thought', 'find', 'found', 'use', 'used', 'work', 'worked',
            'look', 'looked', 'seem', 'seemed', 'feel', 'felt', 'become', 'became',
            'back', 'away', 'up', 'down', 'out', 'in', 'on', 'off',
            'over', 'under', 'through', 'around', 'between', 'among',
            'just', 'only', 'even', 'still', 'yet', 'already', 'now',
            'then', 'here', 'there', 'where', 'when', 'how', 'why',
            'well', 'very', 'too', 'so', 'quite', 'rather', 'really',
            'also', 'again', 'once', 'twice', 'always', 'never', 'often',
            'sometimes', 'usually', 'generally', 'particularly', 'especially'
        ])
        
        # é¢„å¤„ç†è¯­æ–™åº“
        self.processed_corpus = self._preprocess_corpus()
        self.word_doc_freq = self._calculate_word_doc_frequencies()
        self.total_docs = len(self.reference_corpus)
        
        print(f"âœ“ NPMIè®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"  - è¯­æ–™åº“å¤§å°: {self.total_docs} æ–‡æ¡£")
        print(f"  - çª—å£å¤§å°: {self.window_size}")
        print(f"  - åœç”¨è¯æ•°é‡: {len(self.stop_words)}")
    
    def _preprocess_corpus(self) -> List[List[str]]:
        """é¢„å¤„ç†è¯­æ–™åº“ï¼šåˆ†è¯ã€è¯å½¢è¿˜åŸã€å»åœç”¨è¯"""
        processed = []
        
        for doc in self.reference_corpus:
            # åˆ†è¯
            tokens = word_tokenize(doc.lower())
            
            # è¿‡æ»¤å’Œæ ‡å‡†åŒ–
            clean_tokens = []
            for token in tokens:
                # åªä¿ç•™å­—æ¯è¯æ±‡
                if re.match(r'^[a-zA-Z]+$', token) and len(token) > 2:
                    # è¯å½¢è¿˜åŸ
                    lemma = self.lemmatizer.lemmatize(token)
                    # å»åœç”¨è¯
                    if lemma not in self.stop_words:
                        clean_tokens.append(lemma)
            
            processed.append(clean_tokens)
        
        return processed
    
    def _calculate_word_doc_frequencies(self) -> Dict[str, int]:
        """è®¡ç®—è¯æ±‡çš„æ–‡æ¡£é¢‘ç‡"""
        word_doc_freq = defaultdict(int)
        
        for doc_tokens in self.processed_corpus:
            unique_words = set(doc_tokens)
            for word in unique_words:
                word_doc_freq[word] += 1
        
        return dict(word_doc_freq)
    
    def _calculate_window_cooccurrence(self, word1: str, word2: str) -> int:
        """è®¡ç®—ä¸¤ä¸ªè¯åœ¨æ»‘åŠ¨çª—å£å†…çš„å…±ç°æ¬¡æ•°"""
        cooccur_count = 0
        
        for doc_tokens in self.processed_corpus:
            # åœ¨æ¯ä¸ªæ–‡æ¡£ä¸­è®¡ç®—æ»‘åŠ¨çª—å£å…±ç°
            for i in range(len(doc_tokens)):
                if doc_tokens[i] == word1:
                    # æ£€æŸ¥çª—å£èŒƒå›´å†…æ˜¯å¦æœ‰word2
                    start = max(0, i - self.window_size)
                    end = min(len(doc_tokens), i + self.window_size + 1)
                    
                    window_words = doc_tokens[start:end]
                    if word2 in window_words:
                        cooccur_count += 1
                        break  # æ¯ä¸ªæ–‡æ¡£åªè®¡ç®—ä¸€æ¬¡
        
        return cooccur_count
    
    def _calculate_npmi_pair(self, word1: str, word2: str) -> Optional[float]:
        """è®¡ç®—ä¸¤ä¸ªè¯çš„NPMIå€¼"""
        # æ ‡å‡†åŒ–è¯æ±‡
        word1 = self.lemmatizer.lemmatize(word1.lower())
        word2 = self.lemmatizer.lemmatize(word2.lower())
        
        # è·³è¿‡åœç”¨è¯
        if word1 in self.stop_words or word2 in self.stop_words:
            return None
        
        # è·å–è¯é¢‘
        freq_w1 = self.word_doc_freq.get(word1, 0)
        freq_w2 = self.word_doc_freq.get(word2, 0)
        
        if freq_w1 == 0 or freq_w2 == 0:
            return None
        
        # è®¡ç®—å…±ç°é¢‘ç‡
        cooccur_freq = self._calculate_window_cooccurrence(word1, word2)
        
        if cooccur_freq == 0:
            return None
        
        # è®¡ç®—æ¦‚ç‡
        p_w1 = freq_w1 / self.total_docs
        p_w2 = freq_w2 / self.total_docs
        p_w1_w2 = cooccur_freq / self.total_docs
        
        # æ·»åŠ å¹³æ»‘
        epsilon = 1e-10
        p_w1_w2 = max(p_w1_w2, epsilon)
        
        # è®¡ç®—PMI
        pmi = math.log(p_w1_w2 / (p_w1 * p_w2))
        
        # è®¡ç®—NPMI
        npmi = pmi / (-math.log(p_w1_w2))
        
        return npmi
    
    def calculate_topic_npmi(self, topic_words: List[str], topk: int = 10) -> float:
        """è®¡ç®—å•ä¸ªä¸»é¢˜çš„NPMIè¿è´¯æ€§"""
        if len(topic_words) < 2:
            return 0.0
        
        # åªä½¿ç”¨å‰topkä¸ªè¯
        words = topic_words[:topk]
        
        npmi_scores = []
        
        # è®¡ç®—æ‰€æœ‰è¯å¯¹çš„NPMI
        for i in range(len(words)):
            for j in range(i + 1, len(words)):
                npmi = self._calculate_npmi_pair(words[i], words[j])
                if npmi is not None:
                    npmi_scores.append(npmi)
        
        return np.mean(npmi_scores) if npmi_scores else 0.0
    
    def calculate_topics_npmi(self, topics_words: List[List[str]], topk: int = 10) -> float:
        """è®¡ç®—å¤šä¸ªä¸»é¢˜çš„å¹³å‡NPMIè¿è´¯æ€§"""
        if not topics_words:
            return 0.0
        
        topic_scores = []
        
        for i, topic_words in enumerate(topics_words):
            score = self.calculate_topic_npmi(topic_words, topk)
            topic_scores.append(score)
            print(f"  ä¸»é¢˜ {i+1} NPMI: {score:.4f}")
        
        avg_score = np.mean(topic_scores)
        print(f"å¹³å‡NPMIè¿è´¯æ€§: {avg_score:.4f}")
        
        return avg_score
    
    def evaluate_topic_quality(self, topic_words: List[str]) -> Dict[str, any]:
        """è¯„ä¼°ä¸»é¢˜è¯è´¨é‡"""
        # æ ‡å‡†åŒ–è¯æ±‡
        clean_words = []
        stop_word_count = 0
        
        for word in topic_words:
            clean_word = self.lemmatizer.lemmatize(word.lower())
            if clean_word in self.stop_words:
                stop_word_count += 1
            else:
                clean_words.append(clean_word)
        
        # è®¡ç®—æŒ‡æ ‡
        total_words = len(topic_words)
        clean_ratio = len(clean_words) / total_words if total_words > 0 else 0
        stop_ratio = stop_word_count / total_words if total_words > 0 else 0
        
        # è®¡ç®—è¯æ±‡å¤šæ ·æ€§
        unique_words = len(set(clean_words))
        diversity = unique_words / len(clean_words) if clean_words else 0
        
        # è®¡ç®—å¹³å‡è¯é¢‘
        word_freqs = [self.word_doc_freq.get(word, 0) for word in clean_words]
        avg_freq = np.mean(word_freqs) if word_freqs else 0
        
        return {
            'total_words': total_words,
            'clean_words': len(clean_words),
            'stop_words': stop_word_count,
            'clean_ratio': clean_ratio,
            'stop_ratio': stop_ratio,
            'diversity': diversity,
            'avg_frequency': avg_freq,
            'quality_score': clean_ratio * diversity  # ç»¼åˆè´¨é‡åˆ†æ•°
        }
    
    def analyze_topics_quality(self, topics_words: List[List[str]]) -> Dict[str, any]:
        """åˆ†ææ‰€æœ‰ä¸»é¢˜çš„è´¨é‡"""
        quality_scores = []
        detailed_results = []
        
        for i, topic_words in enumerate(topics_words):
            quality = self.evaluate_topic_quality(topic_words)
            quality_scores.append(quality['quality_score'])
            detailed_results.append({
                'topic_id': i + 1,
                'words': topic_words[:10],  # åªæ˜¾ç¤ºå‰10ä¸ªè¯
                **quality
            })
        
        # ç»Ÿè®¡ç»“æœ
        avg_quality = np.mean(quality_scores)
        low_quality_topics = sum(1 for score in quality_scores if score < 0.5)
        
        return {
            'average_quality': avg_quality,
            'low_quality_count': low_quality_topics,
            'total_topics': len(topics_words),
            'detailed_results': detailed_results
        }

def compare_npmi_methods(topics_words: List[List[str]], reference_corpus: List[str]) -> Dict[str, float]:
    """æ¯”è¾ƒä¸åŒNPMIè®¡ç®—æ–¹æ³•çš„ç»“æœ"""
    print("=== NPMIè®¡ç®—æ–¹æ³•æ¯”è¾ƒ ===")
    
    # æ–¹æ³•1ï¼šæ”¹è¿›çš„NPMIè®¡ç®—å™¨
    improved_calculator = ImprovedNPMICalculator(reference_corpus)
    improved_score = improved_calculator.calculate_topics_npmi(topics_words)
    
    # æ–¹æ³•2ï¼šåŸå§‹ç®€å•è®¡ç®—ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
    from topic_evaluation_Tra import TraTopicEvaluator
    original_evaluator = TraTopicEvaluator(reference_corpus)
    original_score = original_evaluator.calculate_npmi_coherence_window(topics_words)
    
    results = {
        'improved_npmi': improved_score,
        'original_npmi': original_score,
        'improvement': improved_score - original_score
    }
    
    print(f"\nğŸ“Š NPMIè®¡ç®—ç»“æœæ¯”è¾ƒ:")
    print(f"  æ”¹è¿›æ–¹æ³•: {improved_score:.4f}")
    print(f"  åŸå§‹æ–¹æ³•: {original_score:.4f}")
    print(f"  æ”¹è¿›å¹…åº¦: {results['improvement']:+.4f}")
    
    return results

if __name__ == "__main__":
    # æµ‹è¯•ç¤ºä¾‹
    test_topics = [
        ["president", "election", "vote", "campaign", "political"],
        ["health", "medical", "hospital", "doctor", "patient"],
        ["economy", "market", "financial", "business", "trade"]
    ]
    
    test_corpus = [
        "The president won the election with a strong campaign.",
        "Medical professionals work in hospitals to help patients.",
        "The economy depends on financial markets and business trade."
    ]
    
    calculator = ImprovedNPMICalculator(test_corpus)
    score = calculator.calculate_topics_npmi(test_topics)
    print(f"æµ‹è¯•NPMIåˆ†æ•°: {score:.4f}")
