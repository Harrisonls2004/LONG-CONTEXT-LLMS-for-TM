#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLMä¸»é¢˜è¯„ä¼°æ¨¡å—
æ•´åˆ prompt_list.pyã€topic_analyzer.py å’Œ topic_evaluation.py çš„åŠŸèƒ½
ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¯¹ä¸»é¢˜è¯åˆ—è¡¨è¿›è¡Œè‡ªåŠ¨åŒ–è¯„ä¼°
"""

import os
import json
import requests
from typing import List, Dict, Any, Optional
from pathlib import Path
import time


class OpenRouterClient:
    """OpenRouter APIå®¢æˆ·ç«¯ - åŸºäº topic_analyzer.py"""

    def __init__(self, api_key: str = None, model: str = "gpt-4"):
        if api_key:
            self.api_key = api_key
        else:
            self.api_key = os.getenv("OPENROUTER_API_KEY")

        self.model = model
        self.base_url = "https://openrouter.ai/api/v1"

        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ°OpenRouter APIå¯†é’¥ï¼Œè¯·è®¾ç½®OPENROUTER_API_KEYç¯å¢ƒå˜é‡æˆ–ç›´æ¥ä¼ å…¥api_keyå‚æ•°")

        print(f"APIå¯†é’¥å·²è®¾ç½®: {self.api_key[:15]}...{self.api_key[-12:]}")
    
    def call_llm(self, prompt: str, max_tokens: int = 4000, temperature: float = 0.7) -> str:
        """è°ƒç”¨OpenRouter API"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://github.com/your-repo",
            "X-Title": "Topic Evaluation LLM"
        }

        data = {
            "model": self.model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": max_tokens,
            "temperature": temperature
        }

        try:
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=data,
                timeout=120
            )
            response.raise_for_status()
            result = response.json()

            if "choices" in result and len(result["choices"]) > 0:
                return result["choices"][0]["message"]["content"]
            else:
                raise Exception(f"APIè¿”å›æ ¼å¼å¼‚å¸¸: {result}")

        except requests.exceptions.RequestException as e:
            if "401" in str(e):
                raise Exception(f"OpenRouter APIè°ƒç”¨å¤±è´¥: 401 Unauthorized - APIå¯†é’¥æ— æ•ˆæˆ–å·²è¿‡æœŸ")
            elif "402" in str(e):
                raise Exception(f"OpenRouter APIè°ƒç”¨å¤±è´¥: 402 Payment Required - è´¦æˆ·ä½™é¢ä¸è¶³")
            elif "429" in str(e):
                raise Exception(f"OpenRouter APIè°ƒç”¨å¤±è´¥: 429 Too Many Requests - è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åé‡è¯•")
            else:
                raise Exception(f"OpenRouter APIè°ƒç”¨å¤±è´¥: {str(e)}")
        except Exception as e:
            raise Exception(f"å¤„ç†å“åº”å¤±è´¥: {str(e)}")


def create_evaluation_prompt(topic_keywords: List[str]) -> str:
    
    prompt = f"""Please evaluate the given topic keyword list based on the following topic quality assessment criteria. For each criterion, provide a score from 1-5 and a brief explanation.

Topic Quality Assessment Criteria:

1. **Coherence (è¯­ä¹‰ä¸€è‡´æ€§)**
   Definition: Keywords within the topic should be semantically closely related and collectively describe a coherent theme or related themes.

2. **Conciseness (ç®€æ´åº¦)**
   Definition: The topic should not contain irrelevant or meaningless words, such as noise words or semantically redundant terms.

3. **Informativity (ä¿¡æ¯å¯†åº¦)**
   Definition: The topic description should contain sufficiently specific, meaningful, or valuable information, covering different aspects of the same theme.

Scoring Guidelines:
- 1 point: Poor performance, does not meet the standard requirements
- 2 points: Below average, partially meets some requirements
- 3 points: Average performance, meets basic requirements
- 4 points: Good performance, exceeds basic requirements
- 5 points: Excellent performance, fully meets all standard requirements

Please evaluate the following topic keyword list:
Topic Keywords: {topic_keywords}

Required Response Format (JSON):
{{
  "topic_keywords": {topic_keywords},
  "evaluation": {{
    "coherence": {{
      "score": <1-5>,
      "explanation": "Brief explanation for the score"
    }},
    "conciseness": {{
      "score": <1-5>,
      "explanation": "Brief explanation for the score"
    }},
    "informativity": {{
      "score": <1-5>,
      "explanation": "Brief explanation for the score"
    }}
  }},
  "overall_score": <average of all scores>,
  "overall_assessment": "Overall assessment and recommendations"
}}

Please provide your evaluation in the exact JSON format specified above."""
    
    return prompt


def parse_llm_evaluation(llm_response: str) -> Dict[str, Any]:
    """è§£æLLMè¿”å›çš„è¯„ä¼°ç»“æœ"""
    try:
        # å°è¯•ç›´æ¥è§£æJSON
        result = json.loads(llm_response)
        return result
    except json.JSONDecodeError:
        # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•æå–JSONéƒ¨åˆ†
        import re
        json_match = re.search(r'\{.*\}', llm_response, re.DOTALL)
        if json_match:
            try:
                result = json.loads(json_match.group())
                return result
            except json.JSONDecodeError:
                pass
        
        # å¦‚æœä»ç„¶å¤±è´¥ï¼Œè¿”å›åŸå§‹å“åº”
        return {
            "error": "Failed to parse LLM response",
            "raw_response": llm_response
        }


def evaluate_topic_with_retry(
    llm_client: OpenRouterClient,
    topic_num: int,
    keywords: List[str],
    max_retries: int = 3
) -> tuple[Dict[str, Any], str, int]:
    """
    å¸¦é‡è¯•æœºåˆ¶çš„ä¸»é¢˜è¯„ä¼°å‡½æ•°
    
    Args:
        llm_client: LLMå®¢æˆ·ç«¯
        topic_num: ä¸»é¢˜ç¼–å·
        keywords: å…³é”®è¯åˆ—è¡¨
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    
    Returns:
        tuple: (è¯„ä¼°ç»“æœ, åŸå§‹LLMå“åº”, å®é™…ä½¿ç”¨çš„å°è¯•æ¬¡æ•°)
    """
    
    for attempt in range(1, max_retries + 1):
        try:
            print(f"ğŸ¤– æ­£åœ¨è°ƒç”¨LLMè¯„ä¼°ä¸»é¢˜ {topic_num} (å°è¯• {attempt}/{max_retries})...")
            print(f"   å…³é”®è¯: {', '.join(keywords[:5])}{'...' if len(keywords) > 5 else ''}")
            
            # åˆ›å»ºè¯„ä¼°æç¤ºè¯
            prompt = create_evaluation_prompt(keywords)
            
            # è°ƒç”¨LLMè¿›è¡Œè¯„ä¼°
            llm_response = llm_client.call_llm(
                prompt, 
                max_tokens=2000, 
                temperature=0.3
            )
            
            print(f"âœ… LLMå“åº”å·²æ¥æ”¶ï¼Œæ­£åœ¨è§£æç»“æœ...")
            
            # è§£æè¯„ä¼°ç»“æœ
            evaluation = parse_llm_evaluation(llm_response)
            
            # æ£€æŸ¥è§£ææ˜¯å¦æˆåŠŸ
            if "error" not in evaluation:
                # è§£ææˆåŠŸï¼Œæ‰“å°è¯„ä¼°ç»“æœæ‘˜è¦
                if "evaluation" in evaluation:
                    scores = []
                    for criterion in ["coherence", "conciseness", "informativity"]:
                        if criterion in evaluation["evaluation"] and "score" in evaluation["evaluation"][criterion]:
                            scores.append(f"{criterion}: {evaluation['evaluation'][criterion]['score']}")
                    print(f"ğŸ“Š ä¸»é¢˜ {topic_num} è¯„ä¼°å®Œæˆ - {', '.join(scores)}")
                else:
                    print(f"ğŸ“Š ä¸»é¢˜ {topic_num} è¯„ä¼°å®Œæˆ")
                
                return evaluation, llm_response, attempt
            else:
                # è§£æå¤±è´¥
                print(f"âŒ ä¸»é¢˜ {topic_num} è§£æå¤±è´¥ (å°è¯• {attempt}/{max_retries}): {evaluation.get('error', 'Unknown error')}")
                
                if attempt < max_retries:
                    print(f"â³ ç­‰å¾…5ç§’åé‡è¯•...")
                    time.sleep(5)
                else:
                    print(f"âŒ ä¸»é¢˜ {topic_num} è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè§£ææœ€ç»ˆå¤±è´¥")
                    return evaluation, llm_response, attempt
                    
        except Exception as e:
            print(f"âŒ ä¸»é¢˜ {topic_num} è¯„ä¼°å‡ºé”™ (å°è¯• {attempt}/{max_retries}): {str(e)}")
            
            if attempt < max_retries:
                print(f"â³ ç­‰å¾…5ç§’åé‡è¯•...")
                time.sleep(5)
            else:
                print(f"âŒ ä¸»é¢˜ {topic_num} è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè¯„ä¼°æœ€ç»ˆå¤±è´¥")
                error_evaluation = {
                    "error": f"Failed after {max_retries} attempts: {str(e)}"
                }
                return error_evaluation, None, attempt
    
    # è¿™é‡Œä¸åº”è¯¥åˆ°è¾¾ï¼Œä½†ä¸ºäº†å®‰å…¨èµ·è§
    return {"error": "Unexpected error in retry logic"}, None, max_retries


def topic_evaluation_LLM(
    topics_data: List[Dict[str, Any]], 
    output_file: str = "llm_topic_evaluation_results.json",
    api_key: str = None,
    model: str = "moonshotai/kimi-k2:free",
    max_topics: Optional[int] = None
) -> Dict[str, Any]:
    """
    ä½¿ç”¨LLMå¯¹ä¸»é¢˜è¯åˆ—è¡¨è¿›è¡Œè¯„ä¼°
    
    Args:
        topics_data: ä¸»é¢˜æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªä¸»é¢˜åŒ…å« 'keywords' å­—æ®µ
        output_file: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
        api_key: OpenRouter APIå¯†é’¥
        model: ä½¿ç”¨çš„LLMæ¨¡å‹
        max_topics: æœ€å¤§è¯„ä¼°ä¸»é¢˜æ•°é‡ï¼ˆç”¨äºæ§åˆ¶æˆæœ¬ï¼‰
    
    Returns:
        åŒ…å«æ‰€æœ‰è¯„ä¼°ç»“æœçš„å­—å…¸
    """
    
    print(f"=== LLMä¸»é¢˜è¯„ä¼°å¼€å§‹ ===")
    print(f"ä½¿ç”¨æ¨¡å‹: {model}")
    print(f"å¾…è¯„ä¼°ä¸»é¢˜æ•°é‡: {len(topics_data)}")
    
    # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
    try:
        llm_client = OpenRouterClient(api_key=api_key, model=model)
    except Exception as e:
        raise Exception(f"åˆå§‹åŒ–LLMå®¢æˆ·ç«¯å¤±è´¥: {str(e)}")
    
    # é™åˆ¶è¯„ä¼°ä¸»é¢˜æ•°é‡ï¼ˆæ§åˆ¶æˆæœ¬ï¼‰
    if max_topics and len(topics_data) > max_topics:
        topics_data = topics_data[:max_topics]
        print(f"é™åˆ¶è¯„ä¼°ä¸»é¢˜æ•°é‡ä¸º: {max_topics}")
    
    evaluation_results = {
        "metadata": {
            "model": model,
            "total_topics": len(topics_data),
            "evaluation_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "evaluation_criteria": [
                "coherence", "conciseness", "informativity"
            ]
        },
        "topic_evaluations": [],
        "summary_statistics": {}
    }
    
    # é€ä¸ªè¯„ä¼°ä¸»é¢˜
    for i, topic in enumerate(topics_data, 1):
        print(f"\næ­£åœ¨è¯„ä¼°ä¸»é¢˜ {i}/{len(topics_data)}...")
        
        # æå–å…³é”®è¯
        keywords = topic.get('keywords', [])
        if not keywords:
            print(f"ä¸»é¢˜ {i} æ²¡æœ‰å…³é”®è¯ï¼Œè·³è¿‡")
            continue
        
        topic_num = topic.get('topic_num', i)
        print(f"ä¸»é¢˜ {topic_num}: {keywords}")
        
        # ä½¿ç”¨å¸¦é‡è¯•æœºåˆ¶çš„è¯„ä¼°å‡½æ•°
        evaluation, llm_response, attempts_used = evaluate_topic_with_retry(
            llm_client=llm_client,
            topic_num=topic_num,
            keywords=keywords,
            max_retries=3
        )
        
        # æ·»åŠ ä¸»é¢˜ä¿¡æ¯
        topic_evaluation = {
            "topic_num": topic_num,
            "keywords": keywords,
            "summary": topic.get('summary', ''),
            "evaluation": evaluation,
            "raw_llm_response": llm_response,
            "attempts_used": attempts_used
        }
        
        evaluation_results["topic_evaluations"].append(topic_evaluation)
        
        # æ·»åŠ å»¶è¿Ÿä»¥é¿å…APIé™åˆ¶ - å¢åŠ åˆ°3ç§’
        time.sleep(3)
    
    # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
    evaluation_results["summary_statistics"] = calculate_summary_statistics(
        evaluation_results["topic_evaluations"]
    )
    
    # ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(evaluation_results, f, ensure_ascii=False, indent=2)
        print(f"\nè¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    except Exception as e:
        print(f"ä¿å­˜ç»“æœæ–‡ä»¶å¤±è´¥: {str(e)}")
    
    # æ‰“å°è¯„ä¼°æŠ¥å‘Š
    print_evaluation_report(evaluation_results)
    
    return evaluation_results


def calculate_summary_statistics(topic_evaluations: List[Dict[str, Any]]) -> Dict[str, Any]:
    """è®¡ç®—æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯"""
    
    valid_evaluations = [
        eval_data for eval_data in topic_evaluations 
        if "error" not in eval_data.get("evaluation", {})
    ]
    
    if not valid_evaluations:
        return {"error": "No valid evaluations found"}
    
    criteria = ["coherence", "conciseness", "informativity"]
    
    statistics = {
        "total_topics_evaluated": len(valid_evaluations),
        "failed_evaluations": len(topic_evaluations) - len(valid_evaluations),
        "average_scores": {},
        "score_distributions": {},
        "overall_average": 0.0
    }
    
    # è®¡ç®—å„ç»´åº¦å¹³å‡åˆ†
    for criterion in criteria:
        scores = []
        for eval_data in valid_evaluations:
            evaluation = eval_data.get("evaluation", {})
            if criterion in evaluation and "score" in evaluation[criterion]:
                scores.append(evaluation[criterion]["score"])
        
        if scores:
            statistics["average_scores"][criterion] = sum(scores) / len(scores)
            statistics["score_distributions"][criterion] = {
                "min": min(scores),
                "max": max(scores),
                "count": len(scores)
            }
    
    # è®°å½•æ¯ä¸ªç»´åº¦çš„å¹³å‡åˆ†
    statistics["dimension_averages"] = {
        "coherence_average": statistics["average_scores"].get("coherence", 0),
        "conciseness_average": statistics["average_scores"].get("conciseness", 0),
        "informativity_average": statistics["average_scores"].get("informativity", 0)
    }
    
    return statistics


def print_evaluation_report(evaluation_results: Dict[str, Any]):
    """æ‰“å°è¯„ä¼°æŠ¥å‘Š"""
    
    print("\n" + "="*80)
    print("ğŸ“Š LLMä¸»é¢˜è¯„ä¼°æŠ¥å‘Š")
    print("="*80)
    
    metadata = evaluation_results.get("metadata", {})
    stats = evaluation_results.get("summary_statistics", {})
    
    print(f"\nğŸ”§ è¯„ä¼°é…ç½®:")
    print(f"   æ¨¡å‹: {metadata.get('model', 'Unknown')}")
    print(f"   è¯„ä¼°æ—¶é—´: {metadata.get('evaluation_time', 'Unknown')}")
    print(f"   æ€»ä¸»é¢˜æ•°: {metadata.get('total_topics', 0)}")
    
    print(f"\nğŸ“ˆ è¯„ä¼°ç»Ÿè®¡:")
    print(f"   æˆåŠŸè¯„ä¼°: {stats.get('total_topics_evaluated', 0)}")
    print(f"   å¤±è´¥è¯„ä¼°: {stats.get('failed_evaluations', 0)}")
    
    if "average_scores" in stats:
        print(f"\nğŸ“Š å„ç»´åº¦å¹³å‡åˆ†:")
        for criterion, score in stats["average_scores"].items():
            print(f"   {criterion.capitalize()}: {score:.2f}/5.0")
    
    print(f"\nğŸ’¡ è¯„ä¼°å»ºè®®:")
    if "average_scores" in stats and stats["average_scores"]:
        avg_score = sum(stats["average_scores"].values()) / len(stats["average_scores"])
        if avg_score >= 4.0:
            print("   - ä¸»é¢˜è´¨é‡ä¼˜ç§€ï¼Œå„ç»´åº¦è¡¨ç°è‰¯å¥½")
        elif avg_score >= 3.0:
            print("   - ä¸»é¢˜è´¨é‡è‰¯å¥½ï¼Œå¯è€ƒè™‘è¿›ä¸€æ­¥ä¼˜åŒ–")
        elif avg_score >= 2.0:
            print("   - ä¸»é¢˜è´¨é‡ä¸€èˆ¬ï¼Œå»ºè®®é‡ç‚¹æ”¹è¿›ä½åˆ†ç»´åº¦")
        else:
            print("   - ä¸»é¢˜è´¨é‡è¾ƒå·®ï¼Œå»ºè®®é‡æ–°ç”Ÿæˆæˆ–å¤§å¹…ä¼˜åŒ–")
    else:
        print("   - ä¸»é¢˜è´¨é‡è¾ƒå·®ï¼Œå»ºè®®é‡æ–°ç”Ÿæˆæˆ–å¤§å¹…ä¼˜åŒ–")


# ç¤ºä¾‹ä½¿ç”¨å‡½æ•°
def example_usage():
    """ç¤ºä¾‹ç”¨æ³•"""
    
    # ç¤ºä¾‹ä¸»é¢˜æ•°æ®ï¼ˆé€šå¸¸æ¥è‡ª topic_analyzer.py çš„è¾“å‡ºï¼‰
    sample_topics = [
        {
            "topic_num": 1,
            "summary": "Financial investment and market analysis",
            "keywords": ["investment", "market", "strategy", "risk", "portfolio", "analysis", "finance"]
        },
        {
            "topic_num": 2,
            "summary": "Technology and innovation",
            "keywords": ["technology", "innovation", "digital", "software", "development", "AI", "automation"]
        }
    ]
    
    # è°ƒç”¨è¯„ä¼°å‡½æ•°
    try:
        results = topic_evaluation_LLM(
            topics_data=sample_topics,
            output_file="example_evaluation_results.json",
            api_key="your-api-key-here",  # æ›¿æ¢ä¸ºå®é™…çš„APIå¯†é’¥
            model="moonshotai/kimi-k2:free",
            max_topics=5  # é™åˆ¶è¯„ä¼°æ•°é‡ä»¥æ§åˆ¶æˆæœ¬
        )
        
        print("\nè¯„ä¼°å®Œæˆï¼")
        return results
        
    except Exception as e:
        print(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        return None


if __name__ == "__main__":
    # è¿è¡Œç¤ºä¾‹
    example_usage()