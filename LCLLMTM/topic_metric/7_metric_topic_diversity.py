#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸»é¢˜é‡å¤ç‡/å¤šæ ·æ€§è¯„ä¼°æŒ‡æ ‡

åˆ†æå•ä¸ªJSONæ–‡ä»¶ä¸­ä¸»é¢˜çš„é‡å¤æ€§å’Œå¤šæ ·æ€§
æŠŠä¸»é¢˜çš„summaryå‘ç»™LLMï¼Œè®©LLMè‡ªåŠ¨è¯†åˆ«é‡å¤çš„ã€å¯åˆå¹¶çš„ä¸»é¢˜å¯¹
"""

import json
import pandas as pd
import numpy as np
from typing import Dict, Any, List
import os
import sys
import re

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥LLMå®¢æˆ·ç«¯
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from topic_analyzer.topic_analyzer import OpenRouterClient
    print("âœ… æˆåŠŸå¯¼å…¥LLMå®¢æˆ·ç«¯")
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥LLMå®¢æˆ·ç«¯: {e}")
    sys.exit(1)

# é…ç½®åŒºåŸŸ
JSON_FILE = "C:/Users/1/Desktop/TopMost/LCLLMTM/LLM_response_results/topic_analysis_NYT_sampled_qwen_qwen3_235b_a22bfree.json"
API_KEY = "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"  # è¯·æ›¿æ¢ä¸ºå®é™…APIå¯†é’¥
MODEL = "moonshotai/kimi-k2:free"

def analyze_topic_diversity(json_file: str, api_key: str, model: str = MODEL) -> Dict[str, Any]:
    """
    åˆ†æå•ä¸ªJSONæ–‡ä»¶ä¸­ä¸»é¢˜çš„é‡å¤ç‡/å¤šæ ·æ€§

    Args:
        json_file (str): ä¸»é¢˜JSONæ–‡ä»¶è·¯å¾„
        api_key (str): APIå¯†é’¥
        model (str): ä½¿ç”¨çš„æ¨¡å‹

    Returns:
        Dict: åˆ†æç»“æœ
    """
    print("ğŸŒˆ åˆ†æä¸»é¢˜é‡å¤ç‡/å¤šæ ·æ€§...")
    print(f"   ğŸ“ JSONæ–‡ä»¶: {json_file}")
    print(f"   ğŸ¤– æ¨¡å‹: {model}")
    
    try:
        # è¯»å–ä¸»é¢˜JSONæ–‡ä»¶
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # æå–ä¸»é¢˜åˆ—è¡¨
        all_topics = []
        if 'topics' in data:
            for i, topic in enumerate(data['topics']):
                # å¤„ç†ä¸åŒçš„ä¸»é¢˜æ•°æ®ç»“æ„
                topic_summary = ""
                topic_id = f"T{i+1}"

                if isinstance(topic, dict):
                    # æ–¹å¼1: ç›´æ¥åŒ…å«summaryå­—æ®µ
                    if 'summary' in topic:
                        topic_summary = topic['summary']
                        topic_id = topic.get('topic_num', f"T{i+1}")

                    # æ–¹å¼2: åµŒå¥—ç»“æ„ï¼Œå¦‚ {"Topic 1": {"Summary": "...", ...}}
                    else:
                        for key, value in topic.items():
                            if isinstance(value, dict) and 'Summary' in value:
                                topic_summary = value['Summary']
                                topic_id = key
                                break
                            elif isinstance(value, dict) and 'summary' in value:
                                topic_summary = value['summary']
                                topic_id = key
                                break

                if topic_summary.strip():
                    all_topics.append({
                        'id': str(topic_id),
                        'summary': topic_summary.strip(),
                        'index': i+1
                    })
        
        if len(all_topics) < 2:
            return {"error": "ä¸»é¢˜æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå¤šæ ·æ€§åˆ†æ"}

        print(f"   ğŸ“Š æˆåŠŸæå–ä¸»é¢˜: {len(all_topics)} ä¸ª")

        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        llm_client = OpenRouterClient(api_key=api_key, model=model)

        # æ„å»ºä¸»é¢˜åˆ—è¡¨æ–‡æœ¬
        topics_text = "\n".join([f"{i+1}. [{topic['id']}] {topic['summary']}" for i, topic in enumerate(all_topics)])

        # æ„å»ºLLMåˆ†ææç¤ºè¯
        prompt = f"""
è¯·åˆ†æä»¥ä¸‹{len(all_topics)}ä¸ªä¸»é¢˜çš„é‡å¤æ€§å’Œå¤šæ ·æ€§ã€‚

ä¸»é¢˜åˆ—è¡¨ï¼š
{topics_text}

è¯·æ‰§è¡Œä»¥ä¸‹åˆ†æä»»åŠ¡ï¼š
1. è¯†åˆ«é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼çš„ä¸»é¢˜å¯¹ï¼ˆç›¸ä¼¼åº¦>75%ï¼‰- è¿™äº›ä¸»é¢˜å¯ä»¥åˆå¹¶
2. è¯†åˆ«éƒ¨åˆ†é‡å çš„ä¸»é¢˜å¯¹ï¼ˆç›¸ä¼¼åº¦40-75%ï¼‰- è¿™äº›ä¸»é¢˜æœ‰ä¸€å®šé‡å ä½†ä»æœ‰åŒºåˆ«
3. è¯„ä¼°ä¸»é¢˜çš„æ•´ä½“å¤šæ ·æ€§ç¨‹åº¦
4. è®¡ç®—å®é™…çš„å”¯ä¸€ä¸»é¢˜æ•°é‡

åˆ†ææ ‡å‡†ï¼š
- é‡å¤ä¸»é¢˜ï¼šå†…å®¹åŸºæœ¬ç›¸åŒï¼Œå¯ä»¥åˆå¹¶ä¸ºä¸€ä¸ªä¸»é¢˜
- éƒ¨åˆ†é‡å ï¼šæœ‰å…±åŒå…ƒç´ ä½†ä¾§é‡ç‚¹ä¸åŒ
- å¤šæ ·æ€§è¯„åˆ†ï¼š0-1ä¹‹é—´ï¼Œ1è¡¨ç¤ºå®Œå…¨å¤šæ ·åŒ–ï¼Œ0è¡¨ç¤ºå®Œå…¨é‡å¤

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
{{
    "redundant_pairs": [
        {{
            "topic1_id": "ä¸»é¢˜1ID",
            "topic1_summary": "ä¸»é¢˜1æ‘˜è¦",
            "topic2_id": "ä¸»é¢˜2ID",
            "topic2_summary": "ä¸»é¢˜2æ‘˜è¦",
            "similarity_score": 0.85,
            "similarity_reason": "ç›¸ä¼¼åŸå› è¯¦ç»†æè¿°",
            "can_merge": true
        }}
    ],
    "partial_overlap_pairs": [
        {{
            "topic1_id": "ä¸»é¢˜1ID",
            "topic2_id": "ä¸»é¢˜2ID",
            "similarity_score": 0.55,
            "overlap_reason": "é‡å åŸå› æè¿°",
            "overlap_areas": ["å…±åŒé¢†åŸŸ1", "å…±åŒé¢†åŸŸ2"]
        }}
    ],
    "unique_topics_count": å®é™…å”¯ä¸€ä¸»é¢˜æ•°é‡,
    "diversity_score": 0åˆ°1ä¹‹é—´çš„å¤šæ ·æ€§å¾—åˆ†,
    "mergeable_groups": [["å¯åˆå¹¶ä¸»é¢˜ID1", "å¯åˆå¹¶ä¸»é¢˜ID2", "å¯åˆå¹¶ä¸»é¢˜ID3"]],
    "analysis_summary": "è¯¦ç»†åˆ†ææ€»ç»“ï¼ŒåŒ…æ‹¬ä¸»è¦å‘ç°å’Œå»ºè®®"
}}
"""
        
        print(f"   ğŸ¤– è°ƒç”¨LLMè¿›è¡Œä¸»é¢˜å¤šæ ·æ€§åˆ†æ...")
        
        # è°ƒç”¨LLMåˆ†æ
        response = llm_client.call_llm(prompt, max_tokens=3000, temperature=0.1)
        
        if not response:
            return {"error": "LLMè°ƒç”¨å¤±è´¥ï¼Œæ— å“åº”"}
        
        # è§£æLLMå“åº”
        try:
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                analysis_result = json.loads(json_match.group())
            else:
                return {"error": "æ— æ³•ä»LLMå“åº”ä¸­æå–JSONç»“æœ"}
        except json.JSONDecodeError:
            return {"error": "LLMå“åº”çš„JSONæ ¼å¼é”™è¯¯"}
        
        # å¤„ç†åˆ†æç»“æœ
        redundant_pairs = analysis_result.get("redundant_pairs", [])
        partial_overlap_pairs = analysis_result.get("partial_overlap_pairs", [])
        unique_count = analysis_result.get("unique_topics_count", len(all_topics))
        diversity_score = analysis_result.get("diversity_score", 0.8)
        mergeable_groups = analysis_result.get("mergeable_groups", [])

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        total_pairs = len(all_topics) * (len(all_topics) - 1) // 2
        redundancy_rate = len(redundant_pairs) / len(all_topics) if all_topics else 0
        overlap_rate = len(partial_overlap_pairs) / total_pairs if total_pairs > 0 else 0

        # è®¡ç®—å¯åˆå¹¶ä¸»é¢˜æ•°é‡
        mergeable_topics_count = sum(len(group) for group in mergeable_groups)
        potential_reduction = len(all_topics) - unique_count

        results = {
            "metric_name": "ä¸»é¢˜é‡å¤ç‡/å¤šæ ·æ€§åˆ†æ",
            "model": model,
            "input_file": json_file,
            "total_topics": len(all_topics),
            "unique_topics": unique_count,
            "redundant_pairs_count": len(redundant_pairs),
            "partial_overlap_pairs_count": len(partial_overlap_pairs),
            "mergeable_groups_count": len(mergeable_groups),
            "mergeable_topics_count": mergeable_topics_count,
            "potential_reduction": potential_reduction,
            "redundancy_rate": round(redundancy_rate, 4),
            "overlap_rate": round(overlap_rate, 4),
            "diversity_score": round(diversity_score, 4),
            "compression_ratio": round(unique_count / len(all_topics), 4) if all_topics else 1.0,
            "redundant_pairs": redundant_pairs,
            "partial_overlap_pairs": partial_overlap_pairs,
            "mergeable_groups": mergeable_groups,
            "llm_analysis": analysis_result.get("analysis_summary", ""),
            "evaluation": {
                "diversity_quality": "ä¼˜ç§€" if diversity_score >= 0.8 else "è‰¯å¥½" if diversity_score >= 0.6 else "ä¸€èˆ¬" if diversity_score >= 0.4 else "è¾ƒå·®",
                "redundancy_severity": "è½»å¾®" if redundancy_rate <= 0.1 else "ä¸­ç­‰" if redundancy_rate <= 0.3 else "ä¸¥é‡",
                "compression_potential": "ä½" if potential_reduction <= 5 else "ä¸­ç­‰" if potential_reduction <= 15 else "é«˜",
                "overall_assessment": "ä¼˜ç§€" if diversity_score >= 0.7 and redundancy_rate <= 0.2 else "è‰¯å¥½" if diversity_score >= 0.5 and redundancy_rate <= 0.4 else "éœ€æ”¹è¿›"
            },
            "topics_sample": all_topics[:10]  # ä¿å­˜å‰10ä¸ªä¸»é¢˜ä½œä¸ºæ ·ä¾‹
        }
        
        # è¾“å‡ºç»“æœ
        print(f"\nğŸŒˆ ä¸»é¢˜é‡å¤ç‡/å¤šæ ·æ€§åˆ†æç»“æœ:")
        print(f"   ğŸ“Š æ€»ä¸»é¢˜æ•°: {results['total_topics']}")
        print(f"   ğŸ“Š å”¯ä¸€ä¸»é¢˜æ•°: {results['unique_topics']}")
        print(f"   ğŸ“Š é‡å¤ä¸»é¢˜å¯¹: {results['redundant_pairs_count']}")
        print(f"   ğŸ“Š éƒ¨åˆ†é‡å å¯¹: {results['partial_overlap_pairs_count']}")
        print(f"   ğŸ“Š å¯åˆå¹¶ç»„æ•°: {results['mergeable_groups_count']}")
        print(f"   ğŸ“Š å¯åˆå¹¶ä¸»é¢˜æ•°: {results['mergeable_topics_count']}")
        print(f"   ğŸ“Š æ½œåœ¨å‹ç¼©æ•°: {results['potential_reduction']}")
        print(f"   ğŸ“Š å†—ä½™ç‡: {results['redundancy_rate']:.2%}")
        print(f"   ğŸ“Š å¤šæ ·æ€§å¾—åˆ†: {results['diversity_score']:.4f}")
        print(f"   ğŸ“Š å‹ç¼©æ¯”: {results['compression_ratio']:.4f}")
        print(f"   ğŸ“Š å¤šæ ·æ€§è´¨é‡: {results['evaluation']['diversity_quality']}")
        print(f"   ğŸ“Š å†—ä½™ä¸¥é‡ç¨‹åº¦: {results['evaluation']['redundancy_severity']}")
        print(f"   ğŸ“Š å‹ç¼©æ½œåŠ›: {results['evaluation']['compression_potential']}")
        print(f"   ğŸ“Š æ•´ä½“è¯„ä¼°: {results['evaluation']['overall_assessment']}")

        # æ˜¾ç¤ºé‡å¤ä¸»é¢˜å¯¹ç¤ºä¾‹
        if redundant_pairs:
            print(f"\nğŸ”„ é‡å¤ä¸»é¢˜å¯¹ç¤ºä¾‹:")
            for i, pair in enumerate(redundant_pairs[:3], 1):
                print(f"   {i}. {pair['topic1_id']} â†” {pair['topic2_id']}")
                print(f"      ç›¸ä¼¼åº¦: {pair.get('similarity_score', 'N/A')}")
                print(f"      åŸå› : {pair.get('similarity_reason', 'N/A')}")

        # æ˜¾ç¤ºå¯åˆå¹¶ç»„ç¤ºä¾‹
        if mergeable_groups:
            print(f"\nğŸ“¦ å¯åˆå¹¶ä¸»é¢˜ç»„ç¤ºä¾‹:")
            for i, group in enumerate(mergeable_groups[:3], 1):
                print(f"   ç»„{i}: {' + '.join(group)}")
        
        return results
        
    except FileNotFoundError as e:
        return {"error": f"æ–‡ä»¶æœªæ‰¾åˆ°: {str(e)}"}
    except json.JSONDecodeError as e:
        return {"error": f"JSONæ–‡ä»¶æ ¼å¼é”™è¯¯: {str(e)}"}
    except Exception as e:
        return {"error": f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"}

def save_results(results: Dict[str, Any], output_file: str = "topic_diversity_results.json"):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = "C:/Users/1/Desktop/TopMost/metric_result"
        os.makedirs(output_dir, exist_ok=True)

        # æ„å»ºå®Œæ•´è·¯å¾„
        full_path = os.path.join(output_dir, output_file)

        with open(full_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {full_path}")
    except Exception as e:
        print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {str(e)}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ä¸»é¢˜é‡å¤ç‡/å¤šæ ·æ€§è¯„ä¼°")
    print("="*60)

    print(f"ğŸ“ ç›®æ ‡æ–‡ä»¶: {JSON_FILE}")
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {MODEL}")

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(JSON_FILE):
        print(f"âŒ JSONæ–‡ä»¶ä¸å­˜åœ¨: {JSON_FILE}")
        return
    else:
        print(f"âœ… JSONæ–‡ä»¶å­˜åœ¨")

    # æ£€æŸ¥APIå¯†é’¥
    if not API_KEY or API_KEY == "your_api_key_here":
        print("âŒ è¯·åœ¨é…ç½®åŒºåŸŸè®¾ç½®æœ‰æ•ˆçš„APIå¯†é’¥")
        return
    else:
        print(f"âœ… APIå¯†é’¥å·²é…ç½®: {API_KEY[:20]}...")

    print(f"\nğŸ”„ å¼€å§‹åˆ†æ...")

    # è¿è¡Œåˆ†æ
    results = analyze_topic_diversity(JSON_FILE, API_KEY, MODEL)

    if "error" in results:
        print(f"âŒ åˆ†æå¤±è´¥: {results['error']}")
        return

    # ä¿å­˜ç»“æœ
    save_results(results)

    print("\nğŸ‰ åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()
