#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸»é¢˜ä¸‹å¹³å‡åˆ†é…æ–‡æœ¬æ•°é‡è¯„ä¼°æŒ‡æ ‡

è¯„ä¼°æ¯ä¸ªä¸»é¢˜ä¸‹åˆ†é…çš„æ–‡æœ¬æ•°é‡ï¼Œè®¡ç®—å‡å€¼å’Œæ–¹å·®
ç”±äºæ— æ³•è¾“å‡ºå…¨éƒ¨æ–‡æ¡£ï¼Œåªèƒ½è¾“å‡ºæœ‰é™æ•°é‡ä¸»é¢˜ä¸‹æ–‡æœ¬
"""

import json
import pandas as pd
import numpy as np
from typing import Dict, Any, List
import os
import sys

# é…ç½®åŒºåŸŸ
INPUT_FILE = "data/NYT_sampled.csv"
JSON_FILE = "llm_analysis/results/topic_analysis_NYT_sampled_qwen_qwen3_235b_a22bfree.json"
def analyze_topic_distribution(json_file: str) -> Dict[str, Any]:
    """
    åˆ†æä¸»é¢˜åˆ†é…åˆ†å¸ƒ
    
    Args:
        json_file (str): ä¸»é¢˜åˆ†æç»“æœJSONæ–‡ä»¶è·¯å¾„
        
    Returns:
        Dict: åˆ†æç»“æœ
    """
    print("ğŸ“Š åˆ†æä¸»é¢˜ä¸‹å¹³å‡åˆ†é…æ–‡æœ¬æ•°é‡...")
    print(f"   ğŸ“ JSONæ–‡ä»¶: {json_file}")
    
    try:
        # è¯»å–JSONæ–‡ä»¶
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # æå–ä¸»é¢˜åˆ†é…ä¿¡æ¯
        topics = data.get('topics', [])
        if not topics:
            return {"error": "JSONæ–‡ä»¶ä¸­æœªæ‰¾åˆ°ä¸»é¢˜ä¿¡æ¯"}
        
        # ç»Ÿè®¡æ¯ä¸ªä¸»é¢˜ä¸‹çš„æ–‡æœ¬æ•°é‡
        topic_text_counts = []
        topic_details = []
        
        for topic in topics:
            if isinstance(topic, dict):
                # è·å–ä¸»é¢˜ä¿¡æ¯
                topic_num = topic.get('topic_num', 'Unknown')
                summary = topic.get('summary', 'No summary')
                
                # ç»Ÿè®¡è¯¥ä¸»é¢˜ä¸‹çš„æ–‡æœ¬æ•°é‡
                source_titles = topic.get('source_titles_with_ids', [])
                text_count = len(source_titles)
                
                topic_text_counts.append(text_count)
                topic_details.append({
                    'topic_num': topic_num,
                    'summary': summary,
                    'text_count': text_count
                })
        
        if not topic_text_counts:
            return {"error": "æœªæ‰¾åˆ°æœ‰æ•ˆçš„ä¸»é¢˜åˆ†é…æ•°æ®"}
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        mean_count = np.mean(topic_text_counts)
        variance = np.var(topic_text_counts)
        std_dev = np.std(topic_text_counts)
        min_count = np.min(topic_text_counts)
        max_count = np.max(topic_text_counts)
        
        # è®¡ç®—åˆ†å¸ƒå‡åŒ€æ€§ï¼ˆå˜å¼‚ç³»æ•°ï¼‰
        cv = std_dev / mean_count if mean_count > 0 else 0
        
        results = {
            "metric_name": "ä¸»é¢˜ä¸‹å¹³å‡åˆ†é…æ–‡æœ¬æ•°é‡",
            "total_topics": len(topic_text_counts),
            "mean_texts_per_topic": round(mean_count, 2),
            "variance": round(variance, 2),
            "standard_deviation": round(std_dev, 2),
            "min_texts_per_topic": int(min_count),
            "max_texts_per_topic": int(max_count),
            "coefficient_of_variation": round(cv, 4),
            "distribution_uniformity": "é«˜" if cv < 0.3 else "ä¸­" if cv < 0.6 else "ä½",
            "topic_details": topic_details,
            "text_count_distribution": topic_text_counts
        }
        
        # è¾“å‡ºç»“æœ
        print(f"\nğŸ“Š ä¸»é¢˜åˆ†é…åˆ†æç»“æœ:")
        print(f"   ğŸ“ˆ æ€»ä¸»é¢˜æ•°: {results['total_topics']}")
        print(f"   ğŸ“ˆ å¹³å‡æ¯ä¸»é¢˜æ–‡æœ¬æ•°: {results['mean_texts_per_topic']}")
        print(f"   ğŸ“ˆ æ–¹å·®: {results['variance']}")
        print(f"   ğŸ“ˆ æ ‡å‡†å·®: {results['standard_deviation']}")
        print(f"   ğŸ“ˆ æœ€å°‘æ–‡æœ¬æ•°: {results['min_texts_per_topic']}")
        print(f"   ğŸ“ˆ æœ€å¤šæ–‡æœ¬æ•°: {results['max_texts_per_topic']}")
        print(f"   ğŸ“ˆ å˜å¼‚ç³»æ•°: {results['coefficient_of_variation']}")
        print(f"   ğŸ“ˆ åˆ†å¸ƒå‡åŒ€æ€§: {results['distribution_uniformity']}")
        
        print(f"\nğŸ“‹ å„ä¸»é¢˜æ–‡æœ¬åˆ†é…è¯¦æƒ… (å®Œæ•´åˆ—è¡¨):")
        for detail in topic_details:  # æ˜¾ç¤ºæ‰€æœ‰ä¸»é¢˜ï¼Œä¸é™åˆ¶æ•°é‡
            print(f"   ä¸»é¢˜{detail['topic_num']}: {detail['text_count']}ç¯‡æ–‡æœ¬ - {detail['summary']}")

        # ç»Ÿè®¡æ–‡æœ¬æ•°é‡åˆ†å¸ƒ
        from collections import Counter
        count_distribution = Counter(topic_text_counts)
        print(f"\nğŸ“Š æ–‡æœ¬æ•°é‡åˆ†å¸ƒç»Ÿè®¡:")
        for count, frequency in sorted(count_distribution.items()):
            print(f"   {count}ç¯‡æ–‡æœ¬: {frequency}ä¸ªä¸»é¢˜")

        # æ˜¾ç¤ºè¯¦ç»†çš„æ•°é‡åˆ†å¸ƒ
        print(f"\nğŸ“ˆ è¯¦ç»†æ•°é‡åˆ†å¸ƒ:")
        print(f"   æ–‡æœ¬æ•°é‡åˆ—è¡¨: {topic_text_counts}")

        # å¦‚æœæœ‰å¼‚å¸¸å€¼ï¼Œç‰¹åˆ«æ ‡å‡º
        if len(set(topic_text_counts)) > 1:
            print(f"\nâš ï¸ å‘ç°ä¸åŒçš„æ–‡æœ¬æ•°é‡:")
            unique_counts = sorted(set(topic_text_counts))
            for count in unique_counts:
                topics_with_count = [i+1 for i, c in enumerate(topic_text_counts) if c == count]
                print(f"   {count}ç¯‡æ–‡æœ¬çš„ä¸»é¢˜: {topics_with_count}")
        else:
            print(f"\nâœ… æ‰€æœ‰ä¸»é¢˜éƒ½æœ‰ç›¸åŒçš„æ–‡æœ¬æ•°é‡: {topic_text_counts[0]}ç¯‡")
        
        return results
        
    except FileNotFoundError:
        return {"error": f"æ–‡ä»¶æœªæ‰¾åˆ°: {json_file}"}
    except json.JSONDecodeError:
        return {"error": f"JSONæ–‡ä»¶æ ¼å¼é”™è¯¯: {json_file}"}
    except Exception as e:
        return {"error": f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"}

def save_results(results: Dict[str, Any], output_file: str = "topic_distribution_results.json"):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = "C:/Users/1/Desktop/TopMost/metric_result"
        os.makedirs(output_dir, exist_ok=True)

        # æ„å»ºå®Œæ•´è·¯å¾„
        full_path = os.path.join(output_dir, output_file)

        with open(full_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {full_path}")
    except Exception as e:
        print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {str(e)}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ä¸»é¢˜ä¸‹å¹³å‡åˆ†é…æ–‡æœ¬æ•°é‡è¯„ä¼°")
    print("="*60)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(JSON_FILE):
        print(f"âŒ JSONæ–‡ä»¶ä¸å­˜åœ¨: {JSON_FILE}")
        return
    
    # è¿è¡Œåˆ†æ
    results = analyze_topic_distribution(JSON_FILE)
    
    if "error" in results:
        print(f"âŒ åˆ†æå¤±è´¥: {results['error']}")
        return
    
    # ä¿å­˜ç»“æœ
    save_results(results)
    
    print("\nğŸ‰ åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()
