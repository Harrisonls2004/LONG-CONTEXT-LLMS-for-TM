#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¾“å…¥æ–‡æœ¬ä½ç½®åå¥½åˆ†æè¯„ä¼°æŒ‡æ ‡

åˆ†åˆ«è®¡ç®—å‰30%ã€ä¸­40%ã€å30%è¾“å…¥æ–‡æœ¬å‡ºç°åœ¨ç­”æ¡ˆä¸­çš„æ¯”ä¾‹
è¯„ä¼°æ¨¡å‹æ˜¯å¦å­˜åœ¨ä½ç½®åå¥½ç°è±¡ï¼ˆå¦‚å‰ç«¯åå¥½ã€ä¸­é—´å¿½è§†ç­‰ï¼‰
"""

import json
import pandas as pd
import numpy as np
from typing import Dict, Any, List
import os
import re

# é…ç½®åŒºåŸŸ
INPUT_FILE = "data/NYT_sampled.csv"
JSON_FILE = "llm_analysis/results/topic_analysis_NYT_sampled_qwen_qwen3_235b_a22bfree.json"

def analyze_position_bias_ratio(csv_file: str, json_file: str) -> Dict[str, Any]:
    """
    åˆ†æè¾“å…¥æ–‡æœ¬ä½ç½®åå¥½æ¯”ä¾‹ï¼ˆå‰30% | ä¸­40% | å30%ï¼‰

    Args:
        csv_file (str): è¾“å…¥CSVæ–‡ä»¶è·¯å¾„
        json_file (str): ç”Ÿæˆç»“æœJSONæ–‡ä»¶è·¯å¾„

    Returns:
        Dict: åˆ†æç»“æœ
    """
    print("ğŸ” åˆ†æè¾“å…¥æ–‡æœ¬ä½ç½®åå¥½æ¯”ä¾‹ï¼ˆå‰30% | ä¸­40% | å30%ï¼‰...")
    print(f"   ğŸ“ CSVæ–‡ä»¶: {csv_file}")
    print(f"   ğŸ“ JSONæ–‡ä»¶: {json_file}")
    
    try:
        # è¯»å–è¾“å…¥CSVæ–‡ä»¶
        df = pd.read_csv(csv_file)
        if 'text' not in df.columns or 'id' not in df.columns:
            return {"error": "CSVæ–‡ä»¶å¿…é¡»åŒ…å«'text'å’Œ'id'åˆ—"}
        
        # è¯»å–ç”Ÿæˆç»“æœJSONæ–‡ä»¶
        with open(json_file, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
        
        # è·å–è¾“å…¥æ–‡æœ¬æ€»æ•°
        total_texts = len(df)

        # è®¡ç®—ä¸‰ä¸ªåŒºé—´çš„æ–‡æœ¬æ•°é‡å’ŒIDé›†åˆ
        front_30_count = int(total_texts * 0.3)
        middle_40_start = front_30_count
        middle_40_end = int(total_texts * 0.7)
        back_30_start = middle_40_end

        # æå–å„åŒºé—´çš„IDé›†åˆ
        front_30_ids = set(df.iloc[:front_30_count]['id'].astype(str))
        middle_40_ids = set(df.iloc[middle_40_start:middle_40_end]['id'].astype(str))
        back_30_ids = set(df.iloc[back_30_start:]['id'].astype(str))

        print(f"   ğŸ“Š æ€»æ–‡æœ¬æ•°: {total_texts}")
        print(f"   ğŸ“Š å‰30%æ–‡æœ¬æ•°: {front_30_count} (ID: 1-{front_30_count})")
        print(f"   ğŸ“Š ä¸­40%æ–‡æœ¬æ•°: {len(middle_40_ids)} (ID: {middle_40_start+1}-{middle_40_end})")
        print(f"   ğŸ“Š å30%æ–‡æœ¬æ•°: {len(back_30_ids)} (ID: {back_30_start+1}-{total_texts})")
        
        # ä»JSONç»“æœä¸­æå–è¢«å¼•ç”¨çš„æ–‡æœ¬ID
        referenced_ids = set()
        topics = json_data.get('topics', [])
        
        for topic in topics:
            if isinstance(topic, dict):
                source_titles = topic.get('source_titles_with_ids', [])
                for source in source_titles:
                    if isinstance(source, dict) and 'id' in source:
                        referenced_ids.add(str(source['id']))
        
        # è®¡ç®—å„åŒºé—´æ–‡æœ¬åœ¨ç»“æœä¸­çš„å‡ºç°æƒ…å†µ
        front_30_referenced = front_30_ids.intersection(referenced_ids)
        middle_40_referenced = middle_40_ids.intersection(referenced_ids)
        back_30_referenced = back_30_ids.intersection(referenced_ids)

        # è®¡ç®—å„åŒºé—´çš„å¼•ç”¨æ¯”ä¾‹
        front_30_ratio = len(front_30_referenced) / front_30_count if front_30_count > 0 else 0
        middle_40_ratio = len(middle_40_referenced) / len(middle_40_ids) if len(middle_40_ids) > 0 else 0
        back_30_ratio = len(back_30_referenced) / len(back_30_ids) if len(back_30_ids) > 0 else 0
        
        # è®¡ç®—ä½ç½®åå¥½æŒ‡æ•°
        front_vs_middle_bias = front_30_ratio / (middle_40_ratio + 0.001)  # å‰ç«¯vsä¸­é—´åå¥½
        back_vs_middle_bias = back_30_ratio / (middle_40_ratio + 0.001)   # åç«¯vsä¸­é—´åå¥½
        front_vs_back_bias = front_30_ratio / (back_30_ratio + 0.001)     # å‰ç«¯vsåç«¯åå¥½

        # è®¡ç®—ä¸­é—´å¿½è§†ç¨‹åº¦
        middle_neglect_score = 1 - middle_40_ratio  # ä¸­é—´å¿½è§†æ¯”ä¾‹ï¼ˆè¶Šé«˜è¶Šä¸¥é‡ï¼‰

        # è®¡ç®—æ•´ä½“ä½ç½®å‡è¡¡æ€§ï¼ˆæ ‡å‡†å·®è¶Šå°è¶Šå‡è¡¡ï¼‰
        position_balance = np.std([front_30_ratio, middle_40_ratio, back_30_ratio])

        results = {
            "metric_name": "è¾“å…¥æ–‡æœ¬ä½ç½®åå¥½åˆ†æï¼ˆå‰30% | ä¸­40% | å30%ï¼‰",
            "total_texts": total_texts,
            "segment_counts": {
                "front_30_count": front_30_count,
                "middle_40_count": len(middle_40_ids),
                "back_30_count": len(back_30_ids)
            },
            "segment_referenced": {
                "front_30_referenced": len(front_30_referenced),
                "middle_40_referenced": len(middle_40_referenced),
                "back_30_referenced": len(back_30_referenced)
            },
            "segment_ratios": {
                "front_30_ratio": round(front_30_ratio, 4),
                "middle_40_ratio": round(middle_40_ratio, 4),
                "back_30_ratio": round(back_30_ratio, 4)
            },
            "bias_indices": {
                "front_vs_middle": round(front_vs_middle_bias, 4),
                "back_vs_middle": round(back_vs_middle_bias, 4),
                "front_vs_back": round(front_vs_back_bias, 4)
            },
            "neglect_scores": {
                "middle_neglect": round(middle_neglect_score, 4),
                "position_balance": round(position_balance, 4)
            },
            "total_referenced": len(referenced_ids),
            "overall_reference_ratio": round(len(referenced_ids) / total_texts, 4),
            "position_analysis": {
                "front_bias": "é«˜" if front_30_ratio > middle_40_ratio * 1.5 else "ä¸­" if front_30_ratio > middle_40_ratio else "ä½",
                "middle_neglect": "ä¸¥é‡" if middle_40_ratio < front_30_ratio * 0.5 else "ä¸­ç­‰" if middle_40_ratio < front_30_ratio * 0.8 else "è½»å¾®",
                "back_bias": "é«˜" if back_30_ratio > middle_40_ratio * 1.5 else "ä¸­" if back_30_ratio > middle_40_ratio else "ä½",
                "overall_balance": "å‡è¡¡" if position_balance < 0.1 else "è½»å¾®åå¥½" if position_balance < 0.2 else "æ˜æ˜¾åå¥½"
            }
        }
        
        # è¾“å‡ºç»“æœ
        print(f"\nğŸ” è¾“å…¥æ–‡æœ¬ä½ç½®åå¥½åˆ†æç»“æœ:")
        print(f"   ğŸ“Š æ€»æ–‡æœ¬æ•°: {results['total_texts']}")
        print(f"   ğŸ“Š å‰30%æ–‡æœ¬å¼•ç”¨ç‡: {results['segment_ratios']['front_30_ratio']:.2%} ({results['segment_referenced']['front_30_referenced']}/{results['segment_counts']['front_30_count']})")
        print(f"   ğŸ“Š ä¸­40%æ–‡æœ¬å¼•ç”¨ç‡: {results['segment_ratios']['middle_40_ratio']:.2%} ({results['segment_referenced']['middle_40_referenced']}/{results['segment_counts']['middle_40_count']})")
        print(f"   ğŸ“Š å30%æ–‡æœ¬å¼•ç”¨ç‡: {results['segment_ratios']['back_30_ratio']:.2%} ({results['segment_referenced']['back_30_referenced']}/{results['segment_counts']['back_30_count']})")
        print(f"   ğŸ“Š ä¸­é—´å¿½è§†å¾—åˆ†: {results['neglect_scores']['middle_neglect']:.4f} (è¶Šä½è¶Šå¥½)")
        print(f"   ğŸ“Š ä½ç½®å‡è¡¡æ€§: {results['neglect_scores']['position_balance']:.4f} (è¶Šä½è¶Šå‡è¡¡)")
        print(f"   ğŸ“Š æ•´ä½“è¯„ä¼°: {results['position_analysis']['overall_balance']}")

        # è¯¦ç»†åˆ†æ
        if front_30_ratio > middle_40_ratio * 1.5:
            print(f"   âš ï¸ æ£€æµ‹åˆ°æ˜æ˜¾çš„å‰ç«¯åå¥½ï¼šå‰30%æ–‡æœ¬å¼•ç”¨ç‡æ˜¾è‘—é«˜äºä¸­é—´éƒ¨åˆ†")

        if back_30_ratio > middle_40_ratio * 1.5:
            print(f"   âš ï¸ æ£€æµ‹åˆ°æ˜æ˜¾çš„åç«¯åå¥½ï¼šå30%æ–‡æœ¬å¼•ç”¨ç‡æ˜¾è‘—é«˜äºä¸­é—´éƒ¨åˆ†")

        if middle_40_ratio < front_30_ratio * 0.5 or middle_40_ratio < back_30_ratio * 0.5:
            print(f"   âš ï¸ æ£€æµ‹åˆ°ä¸­é—´æ–‡æœ¬å¿½è§†ç°è±¡ï¼šä¸­é—´éƒ¨åˆ†å¼•ç”¨ç‡è¿‡ä½")

        # åå¥½æŒ‡æ•°åˆ†æ
        print(f"\nğŸ“ˆ ä½ç½®åå¥½æŒ‡æ•°:")
        print(f"   å‰ç«¯vsä¸­é—´: {results['bias_indices']['front_vs_middle']:.2f}")
        print(f"   åç«¯vsä¸­é—´: {results['bias_indices']['back_vs_middle']:.2f}")
        print(f"   å‰ç«¯vsåç«¯: {results['bias_indices']['front_vs_back']:.2f}")
        
        return results
        
    except FileNotFoundError as e:
        return {"error": f"æ–‡ä»¶æœªæ‰¾åˆ°: {str(e)}"}
    except pd.errors.EmptyDataError:
        return {"error": "CSVæ–‡ä»¶ä¸ºç©º"}
    except json.JSONDecodeError:
        return {"error": f"JSONæ–‡ä»¶æ ¼å¼é”™è¯¯: {json_file}"}
    except Exception as e:
        return {"error": f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"}

def save_results(results: Dict[str, Any], output_file: str = "position_bias_analysis_results.json"):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = "C:/Users/1/Desktop/TopMost/metric_result"
        os.makedirs(output_dir, exist_ok=True)

        # æ„å»ºå®Œæ•´è·¯å¾„
        full_path = os.path.join(output_dir, output_file)

        with open(full_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {full_path}")
    except Exception as e:
        print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {str(e)}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ è¾“å…¥æ–‡æœ¬ä½ç½®åå¥½åˆ†æï¼ˆå‰30% | ä¸­40% | å30%ï¼‰è¯„ä¼°")
    print("="*70)

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {INPUT_FILE}")
        return

    if not os.path.exists(JSON_FILE):
        print(f"âŒ JSONæ–‡ä»¶ä¸å­˜åœ¨: {JSON_FILE}")
        return

    # è¿è¡Œåˆ†æ
    results = analyze_position_bias_ratio(INPUT_FILE, JSON_FILE)
    
    if "error" in results:
        print(f"âŒ åˆ†æå¤±è´¥: {results['error']}")
        return
    
    # ä¿å­˜ç»“æœ
    save_results(results)
    
    print("\nğŸ‰ åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()
