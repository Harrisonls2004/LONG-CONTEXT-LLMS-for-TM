#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŒ‡ä»¤çº¦æŸæ ¼å¼ä¸‹ä¸»é¢˜æ•°é‡ä¸Šé™è¯„ä¼°æŒ‡æ ‡

è¯„ä¼°åœ¨æ»¡è¶³æŒ‡ä»¤è¦æ±‚ä¸‹ï¼ŒLLM å¯ç”Ÿæˆçš„æœ€å¤§ä¸»é¢˜æ•°é‡
è¿™ä¸ªä»»åŠ¡éœ€è¦æ›´æ”¹æç¤ºè¯ï¼Œé¿å…ç”Ÿæˆå›ºå®šæ•°é‡ä¸»é¢˜ï¼Œè€Œæ˜¯æç¤ºæ¨¡å‹ç”Ÿæˆå°½å¯èƒ½å¤šçš„ä¸»é¢˜æ•°é‡
"""

import json
import pandas as pd
import numpy as np
from typing import Dict, Any, List
import os
import sys
import time

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥LLMå®¢æˆ·ç«¯
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from topic_analyzer.topic_analyzer import OpenRouterClient
except ImportError:
    print("âŒ æ— æ³•å¯¼å…¥LLMå®¢æˆ·ç«¯ï¼Œè¯·æ£€æŸ¥è·¯å¾„")
    sys.exit(1)

# é…ç½®åŒºåŸŸ
INPUT_FILE = "data/NYT_sampled.csv"
API_KEY = "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"  # è¯·æ›¿æ¢ä¸ºå®é™…APIå¯†é’¥
MODEL = "qwen/qwen3-235b-a22b:free"
MAX_ATTEMPTS = 3  # æœ€å¤§å°è¯•æ¬¡æ•°
SAMPLE_SIZE = 200  # ç”¨äºæµ‹è¯•çš„æ ·æœ¬å¤§å° - å¢åŠ ä»¥æ”¯æŒæ›´å¤šä¸»é¢˜ç”Ÿæˆ

def measure_max_topic_count(csv_file: str, api_key: str, model: str = MODEL, max_attempts: int = MAX_ATTEMPTS) -> Dict[str, Any]:
    """
    æµ‹é‡æŒ‡ä»¤çº¦æŸä¸‹çš„æœ€å¤§ä¸»é¢˜æ•°é‡
    
    Args:
        csv_file (str): è¾“å…¥CSVæ–‡ä»¶è·¯å¾„
        api_key (str): APIå¯†é’¥
        model (str): ä½¿ç”¨çš„æ¨¡å‹
        max_attempts (int): æœ€å¤§å°è¯•æ¬¡æ•°
        
    Returns:
        Dict: æµ‹é‡ç»“æœ
    """
    print("ğŸ”¢ æµ‹é‡æŒ‡ä»¤çº¦æŸæ ¼å¼ä¸‹ä¸»é¢˜æ•°é‡ä¸Šé™...")
    print(f"   ğŸ“ CSVæ–‡ä»¶: {csv_file}")
    print(f"   ğŸ¤– æ¨¡å‹: {model}")
    print(f"   ğŸ”„ æœ€å¤§å°è¯•æ¬¡æ•°: {max_attempts}")
    
    try:
        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(csv_file)
        if 'text' not in df.columns:
            return {"error": "CSVæ–‡ä»¶å¿…é¡»åŒ…å«'text'åˆ—"}
        
        # éšæœºé‡‡æ ·æ–‡æœ¬ç”¨äºæµ‹è¯•
        sample_texts = df['text'].sample(n=min(SAMPLE_SIZE, len(df))).tolist()
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        llm_client = OpenRouterClient(api_key=api_key, model=model)
        
        # æ„å»ºæµ‹è¯•æ–‡æœ¬
        combined_text = "\n\n".join([f"Document {i+1}: {text}" for i, text in enumerate(sample_texts)])

        print(f"   ğŸ“Š è¾“å…¥æ–‡æœ¬ç»Ÿè®¡:")
        print(f"      æ–‡æœ¬æ€»é•¿åº¦: {len(combined_text):,} å­—ç¬¦")
        print(f"      æ–‡æ¡£æ•°é‡: {len(sample_texts)} ç¯‡")

        # ç¬¬ä¸€æ­¥ï¼šè®©LLMè‡ªå·±ä¼°è®¡èƒ½ç”Ÿæˆå¤šå°‘ä¸ªä¸»é¢˜
        print(f"   ğŸ¤– è®©LLMè‡ªå·±ä¼°è®¡ä¸»é¢˜æ•°é‡...")

        estimation_prompt = f"""
Please analyze the following {len(sample_texts)} news texts and estimate how many distinct topics you can generate from them.

Text content:
{combined_text}

Please provide your estimation in JSON format:
{{
    "estimated_topics_count": your_estimated_number,
    "confidence_level": "high/medium/low",
    "reasoning": "Brief explanation of your estimation"
}}

Be honest about your capabilities and provide a realistic estimate based on the content diversity and your analysis capacity.
"""

        try:
            estimation_response = llm_client.call_llm(estimation_prompt, max_tokens=500, temperature=0.1)

            print(f"   ğŸ“ LLMåŸå§‹å“åº”: {estimation_response[:200]}...")

            # è§£æLLMçš„ä¼°è®¡ - æ›´å¥å£®çš„JSONè§£æ
            import re
            import json

            llm_estimated_count = 0
            confidence_level = 'unknown'
            reasoning = 'No reasoning provided'

            # å°è¯•å¤šç§JSONè§£ææ–¹æ³•
            json_patterns = [
                r'\{[^{}]*"estimated_topics_count"[^{}]*\}',  # ç®€å•JSON
                r'\{.*?"estimated_topics_count".*?\}',        # æ›´å®½æ¾çš„åŒ¹é…
                r'\{.*\}',                                    # æœ€å®½æ¾çš„åŒ¹é…
            ]

            parsed = False
            for pattern in json_patterns:
                json_match = re.search(pattern, estimation_response, re.DOTALL)
                if json_match:
                    try:
                        estimation_data = json.loads(json_match.group())
                        llm_estimated_count = estimation_data.get('estimated_topics_count', 0)
                        confidence_level = estimation_data.get('confidence_level', 'unknown')
                        reasoning = estimation_data.get('reasoning', 'No reasoning provided')
                        parsed = True
                        break
                    except json.JSONDecodeError:
                        continue

            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–æ•°å­—
            if not parsed:
                number_match = re.search(r'(\d+)\s*topics?', estimation_response, re.IGNORECASE)
                if number_match:
                    llm_estimated_count = int(number_match.group(1))
                    confidence_level = 'extracted'
                    reasoning = 'Extracted from text response'
                    parsed = True

            if parsed and llm_estimated_count > 0:
                print(f"   ğŸ¯ LLMè‡ªä¼°ç»“æœ:")
                print(f"      é¢„ä¼°ä¸»é¢˜æ•°: {llm_estimated_count} ä¸ª")
                print(f"      ä¿¡å¿ƒæ°´å¹³: {confidence_level}")
                print(f"      æ¨ç†è¿‡ç¨‹: {reasoning}")
            else:
                llm_estimated_count = 0
                confidence_level = 'unknown'
                reasoning = 'Failed to parse estimation'
                print(f"   âŒ LLMä¼°è®¡è§£æå¤±è´¥")

        except Exception as e:
            llm_estimated_count = 0
            confidence_level = 'unknown'
            reasoning = f'Estimation failed: {str(e)}'
            print(f"   âŒ LLMä¼°è®¡å¤±è´¥: {str(e)}")

        max_topics_generated = 0
        best_result = None
        attempt_results = []
        
        for attempt in range(1, max_attempts + 1):
            print(f"\nğŸ”„ ç¬¬ {attempt} æ¬¡å°è¯•...")
            
            # æ„å»ºæç¤ºè¯ - å¼ºçƒˆè¦æ±‚ç”Ÿæˆå¤§é‡ä¸»é¢˜
            prompt = f"""
CRITICAL INSTRUCTION: You must generate AT LEAST 30-50 topics from the following {len(sample_texts)} news texts. This is a test of your maximum topic generation capacity.

TASK: Analyze the provided texts and extract the MAXIMUM possible number of distinct topics. I expect you to generate 30, 40, 50 or even more topics if the content supports it.

REQUIREMENTS:
1. Generate AT LEAST 30 topics - this is the minimum expectation
2. Aim for 50+ topics if possible - show your full analytical power
3. Include ALL levels of granularity:
   - Broad themes (politics, economy, technology, etc.)
   - Specific subtopics (climate policy, AI healthcare, etc.)
   - Detailed aspects (regulatory frameworks, implementation challenges, etc.)
   - Geographic perspectives (regional impacts, country-specific issues)
   - Temporal aspects (current events, future implications, historical context)
   - Stakeholder perspectives (government, business, citizens, experts)
4. Each topic must be distinct and well-supported by the text
5. Be exhaustive - extract every possible thematic element

TEXT CONTENT:
{combined_text}

OUTPUT FORMAT (JSON only):
{{
    "total_topics": total_number_of_topics,
    "topics": [
        {{
            "topic_num": 1,
            "summary": "Detailed topic description",
            "keywords": ["keyword1", "keyword2", "keyword3", "keyword4"],
            "supporting_docs": ["Document numbers that support this topic"]
        }},
        {{
            "topic_num": 2,
            "summary": "Another detailed topic description",
            "keywords": ["keyword1", "keyword2", "keyword3", "keyword4"],
            "supporting_docs": ["Document numbers that support this topic"]
        }},
        ... continue for ALL topics (aim for 30-50+ topics)
    ]
}}

REMEMBER: This is a capacity test. Generate the MAXIMUM number of topics possible. Do not be conservative - be comprehensive and exhaustive. I want to see your full analytical capabilities!
"""
            
            try:
                # è°ƒç”¨LLM - å¤§å¹…å¢åŠ tokené™åˆ¶ä»¥å…è®¸ç”Ÿæˆæ›´å¤šä¸»é¢˜
                response = llm_client.call_llm(prompt, max_tokens=16000, temperature=0.7)
                
                if not response:
                    print(f"   âŒ ç¬¬ {attempt} æ¬¡å°è¯•å¤±è´¥ï¼šæ— å“åº”")
                    continue
                
                # è§£æJSONå“åº” - æ›´å¥å£®çš„è§£æ
                import re
                import json

                topics_count = 0
                result_data = {}

                # å°è¯•å¤šç§JSONè§£ææ–¹æ³•
                json_patterns = [
                    r'\{[^{}]*"topics"[^{}]*\[.*?\][^{}]*\}',  # åŒ…å«topicsæ•°ç»„çš„JSON
                    r'\{.*?"topics".*?\}',                     # æ›´å®½æ¾çš„åŒ¹é…
                    r'\{.*\}',                                 # æœ€å®½æ¾çš„åŒ¹é…
                ]

                parsed = False
                for pattern in json_patterns:
                    json_match = re.search(pattern, response, re.DOTALL)
                    if json_match:
                        try:
                            result_data = json.loads(json_match.group())
                            topics_count = len(result_data.get('topics', []))
                            parsed = True
                            break
                        except json.JSONDecodeError:
                            continue

                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•è®¡ç®—Topicå…³é”®è¯å‡ºç°æ¬¡æ•°
                if not parsed or topics_count == 0:
                    topic_matches = re.findall(r'"topic[_\s]*(?:num|id|number)"?\s*:\s*\d+', response, re.IGNORECASE)
                    if topic_matches:
                        topics_count = len(topic_matches)
                        result_data = {"topics": [{"topic_num": i+1} for i in range(topics_count)]}
                        parsed = True

                if parsed and topics_count > 0:
                    attempt_result = {
                        "attempt": attempt,
                        "topics_generated": topics_count,
                        "total_topics_claimed": result_data.get('total_topics', topics_count),
                        "response_length": len(response),
                        "success": True
                    }

                    print(f"   âœ… ç¬¬ {attempt} æ¬¡å°è¯•æˆåŠŸï¼šç”Ÿæˆ {topics_count} ä¸ªä¸»é¢˜")

                    if topics_count > max_topics_generated:
                        max_topics_generated = topics_count
                        best_result = result_data

                else:
                    attempt_result = {
                        "attempt": attempt,
                        "topics_generated": 0,
                        "error": "æ— æ³•è§£æJSONå“åº”",
                        "success": False
                    }
                    print(f"   âŒ ç¬¬ {attempt} æ¬¡å°è¯•å¤±è´¥ï¼šæ— æ³•è§£æJSON")
                    print(f"   ğŸ“ å“åº”ç‰‡æ®µ: {response[:200]}...")
                
                attempt_results.append(attempt_result)
                
                # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                if attempt < max_attempts:
                    time.sleep(2)
                    
            except json.JSONDecodeError as e:
                attempt_result = {
                    "attempt": attempt,
                    "topics_generated": 0,
                    "error": f"JSONè§£æé”™è¯¯: {str(e)}",
                    "success": False
                }
                attempt_results.append(attempt_result)
                print(f"   âŒ ç¬¬ {attempt} æ¬¡å°è¯•å¤±è´¥ï¼šJSONè§£æé”™è¯¯")
                
            except Exception as e:
                attempt_result = {
                    "attempt": attempt,
                    "topics_generated": 0,
                    "error": f"è°ƒç”¨é”™è¯¯: {str(e)}",
                    "success": False
                }
                attempt_results.append(attempt_result)
                print(f"   âŒ ç¬¬ {attempt} æ¬¡å°è¯•å¤±è´¥ï¼š{str(e)}")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        successful_attempts = [r for r in attempt_results if r['success']]
        topic_counts = [r['topics_generated'] for r in successful_attempts]

        # è®¡ç®—LLMè‡ªä¼°å‡†ç¡®æ€§
        actual_avg = np.mean(topic_counts) if topic_counts else 0
        actual_max = max_topics_generated

        # è®¡ç®—LLMè‡ªä¼°çš„å‡†ç¡®æ€§
        if llm_estimated_count > 0:
            estimation_accuracy_ratio = round(actual_avg / llm_estimated_count, 2)
            max_accuracy_ratio = round(actual_max / llm_estimated_count, 2)

            if 0.8 <= estimation_accuracy_ratio <= 1.2:
                estimation_quality = "å‡†ç¡®"
            elif estimation_accuracy_ratio < 0.8:
                estimation_quality = "é«˜ä¼°"
            else:
                estimation_quality = "ä½ä¼°"
        else:
            estimation_accuracy_ratio = 0
            max_accuracy_ratio = 0
            estimation_quality = "æ— ä¼°è®¡"

        estimation_accuracy = {
            "llm_estimated_count": llm_estimated_count,
            "confidence_level": confidence_level,
            "reasoning": reasoning,
            "avg_accuracy_ratio": estimation_accuracy_ratio,
            "max_accuracy_ratio": max_accuracy_ratio,
            "estimation_quality": estimation_quality
        }

        results = {
            "metric_name": "æŒ‡ä»¤çº¦æŸæ ¼å¼ä¸‹ä¸»é¢˜æ•°é‡ä¸Šé™",
            "model": model,
            "sample_size": len(sample_texts),
            "input_statistics": {
                "text_length": len(combined_text),
                "document_count": len(sample_texts)
            },
            "llm_self_estimation": {
                "estimated_topics_count": llm_estimated_count,
                "confidence_level": confidence_level,
                "reasoning": reasoning
            },
            "generation_results": {
                "max_attempts": max_attempts,
                "successful_attempts": len(successful_attempts),
                "max_topics_generated": max_topics_generated,
                "average_topics": round(actual_avg, 2),
                "min_topics": min(topic_counts) if topic_counts else 0,
                "topic_generation_stability": round(np.std(topic_counts), 2) if len(topic_counts) > 1 else 0,
                "success_rate": round(len(successful_attempts) / max_attempts, 2)
            },
            "estimation_accuracy": estimation_accuracy,
            "attempt_details": attempt_results,
            "best_result_sample": best_result.get('topics', [])[:5] if best_result else [],  # æ˜¾ç¤ºå‰5ä¸ªä¸»é¢˜ä½œä¸ºæ ·ä¾‹
            "evaluation": {
                "generation_capacity": "ä¼˜ç§€" if max_topics_generated >= 20 else "è‰¯å¥½" if max_topics_generated >= 10 else "ä¸€èˆ¬" if max_topics_generated >= 5 else "è¾ƒä½",
                "stability": "ç¨³å®š" if len(topic_counts) > 1 and np.std(topic_counts) < 3 else "ä¸­ç­‰" if len(topic_counts) > 1 and np.std(topic_counts) < 5 else "ä¸ç¨³å®š",
                "self_estimation_quality": estimation_accuracy["estimation_quality"],
                "overall_assessment": "ä¼˜ç§€" if max_topics_generated >= 15 and len(successful_attempts) >= 2 else "è‰¯å¥½" if max_topics_generated >= 8 else "éœ€æ”¹è¿›"
            }
        }
        
        # è¾“å‡ºç»“æœ
        print(f"\nğŸ”¢ ä¸»é¢˜æ•°é‡ä¸Šé™æµ‹é‡ç»“æœ:")
        print(f"   ğŸ“Š è¾“å…¥ç»Ÿè®¡:")
        print(f"      æ–‡æœ¬é•¿åº¦: {results['input_statistics']['text_length']:,} å­—ç¬¦")
        print(f"      æ–‡æ¡£æ•°é‡: {results['input_statistics']['document_count']} ç¯‡")
        print(f"   ğŸ¤– LLMè‡ªä¼°ç»“æœ:")
        print(f"      é¢„ä¼°ä¸»é¢˜æ•°: {results['llm_self_estimation']['estimated_topics_count']} ä¸ª")
        print(f"      ä¿¡å¿ƒæ°´å¹³: {results['llm_self_estimation']['confidence_level']}")
        print(f"      æ¨ç†è¿‡ç¨‹: {results['llm_self_estimation']['reasoning']}")
        print(f"   ğŸ“ˆ å®é™…ç”Ÿæˆ:")
        print(f"      æœ€å¤§ä¸»é¢˜æ•°: {results['generation_results']['max_topics_generated']}")
        print(f"      å¹³å‡ä¸»é¢˜æ•°: {results['generation_results']['average_topics']}")
        print(f"      æœ€å°‘ä¸»é¢˜æ•°: {results['generation_results']['min_topics']}")
        print(f"   âš–ï¸ è‡ªä¼°å‡†ç¡®æ€§:")
        print(f"      vså¹³å‡å®é™…: {results['estimation_accuracy']['avg_accuracy_ratio']}x")
        print(f"      vsæœ€å¤§å®é™…: {results['estimation_accuracy']['max_accuracy_ratio']}x")
        print(f"      è‡ªä¼°è´¨é‡: {results['estimation_accuracy']['estimation_quality']}")
        print(f"   ğŸ“Š å…¶ä»–æŒ‡æ ‡:")
        print(f"      æˆåŠŸç‡: {results['generation_results']['success_rate']:.0%}")
        print(f"      ç”Ÿæˆç¨³å®šæ€§: {results['generation_results']['topic_generation_stability']}")
        print(f"      ç”Ÿæˆèƒ½åŠ›è¯„ä¼°: {results['evaluation']['generation_capacity']}")
        print(f"      æ•´ä½“è¯„ä¼°: {results['evaluation']['overall_assessment']}")
        
        return results
        
    except FileNotFoundError:
        return {"error": f"æ–‡ä»¶æœªæ‰¾åˆ°: {csv_file}"}
    except pd.errors.EmptyDataError:
        return {"error": "CSVæ–‡ä»¶ä¸ºç©º"}
    except Exception as e:
        return {"error": f"æµ‹é‡è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"}

def save_results(results: Dict[str, Any], output_file: str = "max_topics_results.json"):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = "C:/Users/1/Desktop/TopMost/metric_result"
        os.makedirs(output_dir, exist_ok=True)

        # æ„å»ºå®Œæ•´è·¯å¾„
        full_path = os.path.join(output_dir, output_file)

        with open(full_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {full_path}")
    except Exception as e:
        print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {str(e)}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æŒ‡ä»¤çº¦æŸæ ¼å¼ä¸‹ä¸»é¢˜æ•°é‡ä¸Šé™è¯„ä¼°")
    print("="*60)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {INPUT_FILE}")
        return
    
    # æ£€æŸ¥APIå¯†é’¥
    if not API_KEY or API_KEY == "your_api_key_here":
        print("âŒ è¯·åœ¨é…ç½®åŒºåŸŸè®¾ç½®æœ‰æ•ˆçš„APIå¯†é’¥")
        return
    
    # è¿è¡Œæµ‹é‡
    results = measure_max_topic_count(INPUT_FILE, API_KEY, MODEL, MAX_ATTEMPTS)
    
    if "error" in results:
        print(f"âŒ æµ‹é‡å¤±è´¥: {results['error']}")
        return
    
    # ä¿å­˜ç»“æœ
    save_results(results)
    
    print("\nğŸ‰ æµ‹é‡å®Œæˆï¼")

if __name__ == "__main__":
    main()
