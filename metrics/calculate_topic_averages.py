#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®¡ç®—ds.jsonæ–‡ä»¶ä¸­50ä¸ªä¸»é¢˜çš„coherenceã€conciseã€informativeå¹³å‡å€¼
"""

import json
import statistics

def calculate_topic_averages(json_file_path):
    """è®¡ç®—ä¸»é¢˜è¯„ä¼°çš„å¹³å‡å€¼"""
    
    print(f"ğŸ“Š åˆ†ææ–‡ä»¶: {json_file_path}")
    print("="*60)
    
    try:
        # è¯»å–JSONæ–‡ä»¶
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # æå–åŸºæœ¬ä¿¡æ¯
        metadata = data.get('metadata', {})
        topic_evaluations = data.get('topic_evaluations', [])
        
        print(f"ğŸ“‹ æ–‡ä»¶ä¿¡æ¯:")
        print(f"   æ¨¡å‹: {metadata.get('model', 'Unknown')}")
        print(f"   ä¸»é¢˜æ€»æ•°: {metadata.get('total_topics', len(topic_evaluations))}")
        print(f"   è¯„ä¼°æ—¶é—´: {metadata.get('evaluation_time', 'Unknown')}")
        print(f"   å®é™…ä¸»é¢˜æ•°: {len(topic_evaluations)}")
        print()
        
        # æ”¶é›†æ‰€æœ‰è¯„åˆ†
        coherence_scores = []
        conciseness_scores = []
        informativity_scores = []
        overall_scores = []
        
        valid_topics = 0
        invalid_topics = 0
        
        for i, topic in enumerate(topic_evaluations, 1):
            try:
                evaluation = topic.get('evaluation', {}).get('evaluation', {})
                
                # æå–å„é¡¹è¯„åˆ†
                coherence = evaluation.get('coherence', {}).get('score')
                conciseness = evaluation.get('conciseness', {}).get('score')
                informativity = evaluation.get('informativity', {}).get('score')
                overall = topic.get('evaluation', {}).get('overall_score')
                
                # éªŒè¯è¯„åˆ†æœ‰æ•ˆæ€§
                if all(score is not None for score in [coherence, conciseness, informativity, overall]):
                    coherence_scores.append(coherence)
                    conciseness_scores.append(conciseness)
                    informativity_scores.append(informativity)
                    overall_scores.append(overall)
                    valid_topics += 1
                else:
                    print(f"âš ï¸  ä¸»é¢˜ {i} è¯„åˆ†æ•°æ®ä¸å®Œæ•´ï¼Œè·³è¿‡")
                    invalid_topics += 1
                    
            except Exception as e:
                print(f"âŒ ä¸»é¢˜ {i} æ•°æ®è§£æé”™è¯¯: {e}")
                invalid_topics += 1
        
        print(f"ğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
        print(f"   æœ‰æ•ˆä¸»é¢˜: {valid_topics}")
        print(f"   æ— æ•ˆä¸»é¢˜: {invalid_topics}")
        print()
        
        if not coherence_scores:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è¯„åˆ†æ•°æ®")
            return None
        
        # è®¡ç®—å¹³å‡å€¼å’Œå…¶ä»–ç»Ÿè®¡ä¿¡æ¯
        results = {
            'coherence': {
                'mean': statistics.mean(coherence_scores),
                'median': statistics.median(coherence_scores),
                'stdev': statistics.stdev(coherence_scores) if len(coherence_scores) > 1 else 0,
                'min': min(coherence_scores),
                'max': max(coherence_scores),
                'count': len(coherence_scores)
            },
            'conciseness': {
                'mean': statistics.mean(conciseness_scores),
                'median': statistics.median(conciseness_scores),
                'stdev': statistics.stdev(conciseness_scores) if len(conciseness_scores) > 1 else 0,
                'min': min(conciseness_scores),
                'max': max(conciseness_scores),
                'count': len(conciseness_scores)
            },
            'informativity': {
                'mean': statistics.mean(informativity_scores),
                'median': statistics.median(informativity_scores),
                'stdev': statistics.stdev(informativity_scores) if len(informativity_scores) > 1 else 0,
                'min': min(informativity_scores),
                'max': max(informativity_scores),
                'count': len(informativity_scores)
            },
            'overall': {
                'mean': statistics.mean(overall_scores),
                'median': statistics.median(overall_scores),
                'stdev': statistics.stdev(overall_scores) if len(overall_scores) > 1 else 0,
                'min': min(overall_scores),
                'max': max(overall_scores),
                'count': len(overall_scores)
            }
        }
        
        # æ˜¾ç¤ºç»“æœ
        print("ğŸ¯ è¯„ä¼°ç»“æœç»Ÿè®¡:")
        print("="*60)
        
        for metric, stats in results.items():
            print(f"\nğŸ“Š {metric.upper()}:")
            print(f"   å¹³å‡å€¼: {stats['mean']:.3f}")
            print(f"   ä¸­ä½æ•°: {stats['median']:.3f}")
            print(f"   æ ‡å‡†å·®: {stats['stdev']:.3f}")
            print(f"   æœ€å°å€¼: {stats['min']}")
            print(f"   æœ€å¤§å€¼: {stats['max']}")
            print(f"   æ ·æœ¬æ•°: {stats['count']}")
        
        # ç®€æ´æ€»ç»“
        print(f"\nğŸ¯ ç®€æ´æ€»ç»“:")
        print("="*60)
        print(f"Coherence å¹³å‡å€¼:    {results['coherence']['mean']:.3f}")
        print(f"Conciseness å¹³å‡å€¼:  {results['conciseness']['mean']:.3f}")
        print(f"Informativity å¹³å‡å€¼: {results['informativity']['mean']:.3f}")
        print(f"Overall å¹³å‡å€¼:      {results['overall']['mean']:.3f}")
        
        # è¯„ä¼°è´¨é‡ç­‰çº§
        print(f"\nğŸ“ˆ è´¨é‡è¯„ä¼°:")
        print("="*60)
        
        def get_quality_level(score):
            if score >= 4.5:
                return "ä¼˜ç§€ (Excellent)"
            elif score >= 4.0:
                return "è‰¯å¥½ (Good)"
            elif score >= 3.5:
                return "ä¸­ç­‰ (Average)"
            elif score >= 3.0:
                return "ä¸€èˆ¬ (Below Average)"
            else:
                return "è¾ƒå·® (Poor)"
        
        print(f"Coherence:    {get_quality_level(results['coherence']['mean'])}")
        print(f"Conciseness:  {get_quality_level(results['conciseness']['mean'])}")
        print(f"Informativity: {get_quality_level(results['informativity']['mean'])}")
        print(f"Overall:      {get_quality_level(results['overall']['mean'])}")
        
        return results
        
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {json_file_path}")
        return None
    except json.JSONDecodeError as e:
        print(f"âŒ JSONè§£æé”™è¯¯: {e}")
        return None
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return None

def main():
    """ä¸»å‡½æ•°"""
    json_file = r"traditional_models/results/ds.json"
    
    print("ğŸ” ä¸»é¢˜è¯„ä¼°å¹³å‡å€¼è®¡ç®—å™¨")
    print("="*60)
    
    results = calculate_topic_averages(json_file)
    
    if results:
        print(f"\nâœ… è®¡ç®—å®Œæˆï¼")
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        output_file = "topic_averages_summary.json"
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            print(f"ğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
    else:
        print(f"\nâŒ è®¡ç®—å¤±è´¥")

if __name__ == "__main__":
    main()
