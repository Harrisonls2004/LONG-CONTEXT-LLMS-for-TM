#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¿è¡Œæ‰€æœ‰LLMä¸»é¢˜å»ºæ¨¡è¯„ä¼°æŒ‡æ ‡

è¿™ä¸ªè„šæœ¬ä¼šä¾æ¬¡è¿è¡Œæ‰€æœ‰6ä¸ªç‹¬ç«‹çš„æŒ‡æ ‡è¯„ä¼°æ–‡ä»¶
"""

import os
import sys
import subprocess
import json
import time
from datetime import datetime
from typing import Dict, Any

# æŒ‡æ ‡æ–‡ä»¶åˆ—è¡¨
METRIC_FILES = [
    {
        "name": "ä¸»é¢˜ä¸‹å¹³å‡åˆ†é…æ–‡æœ¬æ•°é‡",
        "file": "metric_topic_distribution.py",
        "description": "è¯„ä¼°æ¯ä¸ªä¸»é¢˜ä¸‹åˆ†é…çš„æ–‡æœ¬æ•°é‡åˆ†å¸ƒ"
    },
    {
        "name": "é«˜é¢‘ä¸»é¢˜ä¼˜å…ˆç”Ÿæˆ",
        "file": "metric_high_frequency_priority.py", 
        "description": "åˆ¤æ–­æ˜¯å¦ä¼˜å…ˆç”Ÿæˆé«˜é¢‘ä¸»é¢˜"
    },
    {
        "name": "è¾“å…¥æ–‡æœ¬å¿½è§†æ¯”ï¼ˆå‰30%ï¼‰",
        "file": "metric_input_neglect.py",
        "description": "è¯„ä¼°å‰30%è¾“å…¥æ–‡æœ¬çš„å¿½è§†æƒ…å†µ"
    },
    {
        "name": "æŒ‡ä»¤çº¦æŸä¸‹ä¸»é¢˜æ•°é‡ä¸Šé™",
        "file": "metric_max_topics.py",
        "description": "æµ‹é‡LLMå¯ç”Ÿæˆçš„æœ€å¤§ä¸»é¢˜æ•°é‡"
    },
    {
        "name": "ä¸»é¢˜æ–‡æœ¬ç›¸å…³æ€§/å¹»è§‰ç‡",
        "file": "metric_topic_relevance.py",
        "description": "è¯„ä¼°ä¸»é¢˜å…³é”®è¯ä¸æ–‡æœ¬çš„ç›¸å…³æ€§"
    },
    {
        "name": "ä¸»é¢˜é‡å¤ç‡/å¤šæ ·æ€§",
        "file": "metric_topic_diversity.py",
        "description": "åˆ†æä¸»é¢˜çš„é‡å¤ç‡å’Œå¤šæ ·æ€§"
    }
]

def run_single_metric(metric_file: str, metric_name: str) -> Dict[str, Any]:
    """
    è¿è¡Œå•ä¸ªæŒ‡æ ‡è¯„ä¼°
    
    Args:
        metric_file (str): æŒ‡æ ‡æ–‡ä»¶å
        metric_name (str): æŒ‡æ ‡åç§°
        
    Returns:
        Dict: è¿è¡Œç»“æœ
    """
    print(f"\n{'='*60}")
    print(f"ğŸš€ è¿è¡ŒæŒ‡æ ‡: {metric_name}")
    print(f"ğŸ“ æ–‡ä»¶: {metric_file}")
    print(f"{'='*60}")
    
    start_time = time.time()
    
    try:
        # è¿è¡ŒæŒ‡æ ‡æ–‡ä»¶
        result = subprocess.run(
            [sys.executable, metric_file],
            capture_output=True,
            text=True,
            cwd=os.path.dirname(os.path.abspath(__file__))
        )
        
        end_time = time.time()
        duration = end_time - start_time
        
        if result.returncode == 0:
            print(f"âœ… {metric_name} è¿è¡ŒæˆåŠŸ")
            print(f"â±ï¸ è€—æ—¶: {duration:.2f} ç§’")
            
            # å°è¯•è¯»å–ç»“æœæ–‡ä»¶
            result_files = {
                "metric_topic_distribution.py": "topic_distribution_results.json",
                "metric_high_frequency_priority.py": "high_frequency_priority_results.json",
                "metric_input_neglect.py": "input_neglect_results.json",
                "metric_max_topics.py": "max_topics_results.json",
                "metric_topic_relevance.py": "topic_relevance_results.json",
                "metric_topic_diversity.py": "topic_diversity_results.json"
            }

            result_file = result_files.get(metric_file)
            metric_result = None

            if result_file:
                # æ„å»ºå®Œæ•´è·¯å¾„
                result_dir = "C:/Users/1/Desktop/TopMost/metric_result"
                full_result_path = os.path.join(result_dir, result_file)

                if os.path.exists(full_result_path):
                    try:
                        with open(full_result_path, 'r', encoding='utf-8') as f:
                            metric_result = json.load(f)
                    except Exception as e:
                        print(f"âš ï¸ è¯»å–ç»“æœæ–‡ä»¶å¤±è´¥: {str(e)}")
            
            return {
                "metric_name": metric_name,
                "file": metric_file,
                "status": "success",
                "duration": duration,
                "stdout": result.stdout,
                "result_data": metric_result
            }
        else:
            print(f"âŒ {metric_name} è¿è¡Œå¤±è´¥")
            print(f"é”™è¯¯è¾“å‡º: {result.stderr}")
            
            return {
                "metric_name": metric_name,
                "file": metric_file,
                "status": "failed",
                "duration": duration,
                "error": result.stderr,
                "stdout": result.stdout
            }
            
    except Exception as e:
        end_time = time.time()
        duration = end_time - start_time
        
        print(f"âŒ {metric_name} è¿è¡Œå¼‚å¸¸: {str(e)}")
        
        return {
            "metric_name": metric_name,
            "file": metric_file,
            "status": "error",
            "duration": duration,
            "error": str(e)
        }

def generate_summary_report(results: list) -> Dict[str, Any]:
    """
    ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    
    Args:
        results (list): æ‰€æœ‰æŒ‡æ ‡çš„è¿è¡Œç»“æœ
        
    Returns:
        Dict: æ±‡æ€»æŠ¥å‘Š
    """
    successful_metrics = [r for r in results if r['status'] == 'success']
    failed_metrics = [r for r in results if r['status'] != 'success']
    
    total_duration = sum(r['duration'] for r in results)
    
    summary = {
        "evaluation_timestamp": datetime.now().isoformat(),
        "total_metrics": len(results),
        "successful_metrics": len(successful_metrics),
        "failed_metrics": len(failed_metrics),
        "success_rate": len(successful_metrics) / len(results) if results else 0,
        "total_duration": round(total_duration, 2),
        "average_duration": round(total_duration / len(results), 2) if results else 0,
        "metric_results": results,
        "summary_statistics": {}
    }
    
    # æå–å…³é”®ç»Ÿè®¡ä¿¡æ¯
    for result in successful_metrics:
        if result.get('result_data'):
            metric_name = result['metric_name']
            data = result['result_data']
            
            # æ ¹æ®ä¸åŒæŒ‡æ ‡æå–å…³é”®ä¿¡æ¯
            if "ä¸»é¢˜ä¸‹å¹³å‡åˆ†é…æ–‡æœ¬æ•°é‡" in metric_name:
                summary['summary_statistics']['topic_distribution'] = {
                    "mean_texts_per_topic": data.get('mean_texts_per_topic'),
                    "distribution_uniformity": data.get('distribution_uniformity')
                }
            elif "é«˜é¢‘ä¸»é¢˜ä¼˜å…ˆç”Ÿæˆ" in metric_name:
                summary['summary_statistics']['high_frequency_priority'] = {
                    "priority_score": data.get('priority_score'),
                    "stability_score": data.get('stability_score'),
                    "overall_assessment": data.get('evaluation', {}).get('overall_assessment')
                }
            elif "è¾“å…¥æ–‡æœ¬å¿½è§†æ¯”" in metric_name:
                summary['summary_statistics']['input_neglect'] = {
                    "front_30_percent_ratio": data.get('front_30_percent_ratio'),
                    "neglect_score": data.get('neglect_score'),
                    "overall_assessment": data.get('position_analysis', {}).get('overall_assessment')
                }
            elif "ä¸»é¢˜æ•°é‡ä¸Šé™" in metric_name:
                summary['summary_statistics']['max_topics'] = {
                    "max_topics_generated": data.get('max_topics_generated'),
                    "generation_capacity": data.get('evaluation', {}).get('generation_capacity')
                }
            elif "ä¸»é¢˜æ–‡æœ¬ç›¸å…³æ€§" in metric_name:
                summary['summary_statistics']['topic_relevance'] = {
                    "mean_relevance_score": data.get('mean_relevance_score'),
                    "mean_hallucination_ratio": data.get('mean_hallucination_ratio'),
                    "overall_assessment": data.get('evaluation', {}).get('overall_assessment')
                }
            elif "ä¸»é¢˜é‡å¤ç‡" in metric_name:
                summary['summary_statistics']['topic_diversity'] = {
                    "diversity_score": data.get('diversity_score'),
                    "redundancy_rate": data.get('redundancy_rate'),
                    "overall_assessment": data.get('evaluation', {}).get('overall_assessment')
                }
    
    return summary

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ LLMä¸»é¢˜å»ºæ¨¡è¯„ä¼°æŒ‡æ ‡ - æ‰¹é‡è¿è¡Œ")
    print("="*80)
    print(f"ğŸ“Š å°†è¿è¡Œ {len(METRIC_FILES)} ä¸ªè¯„ä¼°æŒ‡æ ‡")
    print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # æ˜¾ç¤ºæŒ‡æ ‡åˆ—è¡¨
    print(f"\nğŸ“‹ æŒ‡æ ‡åˆ—è¡¨:")
    for i, metric in enumerate(METRIC_FILES, 1):
        print(f"   {i}. {metric['name']} - {metric['description']}")
    
    # è¯¢é—®æ˜¯å¦ç»§ç»­
    response = input(f"\næ˜¯å¦ç»§ç»­è¿è¡Œæ‰€æœ‰æŒ‡æ ‡ï¼Ÿ(y/n): ").strip().lower()
    if response not in ['y', 'yes', 'æ˜¯']:
        print("âŒ ç”¨æˆ·å–æ¶ˆè¿è¡Œ")
        return
    
    # è¿è¡Œæ‰€æœ‰æŒ‡æ ‡
    results = []
    
    for i, metric in enumerate(METRIC_FILES, 1):
        print(f"\nğŸ“ˆ è¿›åº¦: {i}/{len(METRIC_FILES)}")
        
        result = run_single_metric(metric['file'], metric['name'])
        results.append(result)
        
        # çŸ­æš‚æš‚åœé¿å…èµ„æºå†²çª
        if i < len(METRIC_FILES):
            time.sleep(1)
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    print(f"\n{'='*80}")
    print("ğŸ“Š ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š...")
    
    summary_report = generate_summary_report(results)
    
    # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
    report_file = f"llm_metrics_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = "C:/Users/1/Desktop/TopMost/metric_result"
        os.makedirs(output_dir, exist_ok=True)

        # æ„å»ºå®Œæ•´è·¯å¾„
        full_report_path = os.path.join(output_dir, report_file)

        with open(full_report_path, 'w', encoding='utf-8') as f:
            json.dump(summary_report, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {full_report_path}")
    except Exception as e:
        print(f"âŒ ä¿å­˜æ±‡æ€»æŠ¥å‘Šå¤±è´¥: {str(e)}")
    
    # æ˜¾ç¤ºè¿è¡Œç»“æœ
    print(f"\nğŸ‰ æ‰¹é‡è¿è¡Œå®Œæˆï¼")
    print(f"ğŸ“Š æ€»æŒ‡æ ‡æ•°: {summary_report['total_metrics']}")
    print(f"âœ… æˆåŠŸè¿è¡Œ: {summary_report['successful_metrics']}")
    print(f"âŒ è¿è¡Œå¤±è´¥: {summary_report['failed_metrics']}")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {summary_report['success_rate']:.1%}")
    print(f"â±ï¸ æ€»è€—æ—¶: {summary_report['total_duration']:.2f} ç§’")
    
    # æ˜¾ç¤ºå¤±è´¥çš„æŒ‡æ ‡
    if summary_report['failed_metrics'] > 0:
        print(f"\nâŒ å¤±è´¥çš„æŒ‡æ ‡:")
        for result in results:
            if result['status'] != 'success':
                print(f"   - {result['metric_name']}: {result.get('error', 'Unknown error')}")
    
    print(f"\nğŸ“ è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹å„ä¸ªæŒ‡æ ‡ç”Ÿæˆçš„ç»“æœæ–‡ä»¶å’Œæ±‡æ€»æŠ¥å‘Š: {full_report_path}")

if __name__ == "__main__":
    main()
