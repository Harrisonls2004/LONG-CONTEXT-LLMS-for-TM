#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NYTæ•°æ®é›†ä¸»é¢˜å±‚æ¬¡åŒ–å¤„ç†è„šæœ¬
ä½¿ç”¨LLMä¸ºæ¯è¡Œæ•°æ®ç”ŸæˆäºŒ/ä¸‰çº§ä¸»é¢˜ï¼Œå¹¶æ·»åŠ åˆ°CSVæ–‡ä»¶çš„æ–°åˆ—ä¸­
"""

import os
import csv
import json
import time
import pandas as pd
from typing import List, Dict, Any, Optional
from pathlib import Path
from openai import OpenAI

# å¯¼å…¥é…ç½®
try:
    from config import (
        RECOMMENDED_MODELS, DEFAULT_MODEL, RATE_LIMIT_INTERVAL,
        MAX_RETRIES, DEFAULT_MAX_TOKENS, DEFAULT_TEMPERATURE,
        PROMPT_TEMPLATE_FOCUSED, PROMPT_TEMPLATE_MODERATE, PROMPT_TEMPLATE_STRICT, PROMPT_TEMPLATE_MAXIMUM,
        TEXT_PREVIEW_LENGTH, TOPIC_SEPARATOR,
        ERROR_MARKER, DEFAULT_ENCODING_OPTIONS
    )
except ImportError:
    # å¦‚æœæ²¡æœ‰config.pyï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    RECOMMENDED_MODELS = [
        "qwen/qwen3-14b:free",
        "qwen/qwen3-coder:free",
        "meta-llama/llama-3.3-70b-instruct:free",
        "google/gemini-2.0-flash-exp:free",
        "deepseek/deepseek-r1-0528:free",
        "deepseek/deepseek-chat-v3.1:free"
    ]
    DEFAULT_MODEL = "qwen/qwen3-14b:free"
    RATE_LIMIT_INTERVAL = 2.0
    MAX_RETRIES = 3
    DEFAULT_MAX_TOKENS = 2000
    DEFAULT_TEMPERATURE = 0.3
    TEXT_PREVIEW_LENGTH = 1000
    TOPIC_SEPARATOR = '; '
    ERROR_MARKER = 'ERROR'
    DEFAULT_ENCODING_OPTIONS = ['utf-8', 'gbk', 'latin-1', 'cp1252']

    # é»˜è®¤æç¤ºè¯æ¨¡æ¿ï¼ˆè‹±æ–‡ç‰ˆæœ¬ï¼‰
    PROMPT_TEMPLATE = """You are a professional news topic analysis expert. Please generate a detailed topic hierarchy for the following news article.

**Article Information:**
Title: {title}
Primary Topic: {primary_topic}
Keywords: {keywords}
Content: {text_preview}...

**Requirements:**
1. Based on the article content, generate as many secondary and tertiary topics as possible
2. Do not limit the number of topics - generate all relevant topics
3. Topics should be specific, accurate, and reflect different aspects of the article
4. Secondary topics should be subdivisions of the primary topic
5. Tertiary topics should be further subdivisions of secondary topics

**Output Format (strictly follow JSON format):**
```json
{{
    "secondary_topics": [
        "Specific secondary topic 1",
        "Specific secondary topic 2",
        "Specific secondary topic 3"
    ],
    "tertiary_topics": [
        "Specific tertiary topic 1",
        "Specific tertiary topic 2",
        "Specific tertiary topic 3",
        "Specific tertiary topic 4"
    ]
}}
```

Please ensure:
- The number of generated topics is not fixed, determined by the richness of article content
- Topic descriptions are concise and clear, typically 2-6 words
- All topics are highly relevant to the article content
- Output must be valid JSON format"""


class OpenRouterClient:
    """OpenRouter APIå®¢æˆ·ç«¯ - å‚è€ƒtopic_evaluator.pyçš„å®ç°"""

    def __init__(self, api_key: str = None, model: str = None):
        if api_key:
            self.api_key = api_key
        else:
            self.api_key = os.getenv("OPENROUTER_API_KEY")

        self.model = model or DEFAULT_MODEL
        self.base_url = "https://openrouter.ai/api/v1"

        # æ·»åŠ é€Ÿç‡é™åˆ¶ç›¸å…³å±æ€§
        self.last_request_time = 0
        self.min_request_interval = RATE_LIMIT_INTERVAL

        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ°OpenRouter APIå¯†é’¥ï¼Œè¯·è®¾ç½®OPENROUTER_API_KEYç¯å¢ƒå˜é‡æˆ–ç›´æ¥ä¼ å…¥api_keyå‚æ•°")

        # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯è¿æ¥OpenRouter
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            default_headers={
                "HTTP-Referer": "https://github.com/topic-hierarchy-generator",
                "X-Title": "NYT Topic Hierarchy Generator",
            },
        )

        print(f"APIå¯†é’¥å·²è®¾ç½®: {self.api_key[:15]}...{self.api_key[-12:]}")
        print(f"ä½¿ç”¨æ¨¡å‹: {self.model}")
        print(f"é€Ÿç‡é™åˆ¶: æœ€å°è¯·æ±‚é—´éš” {self.min_request_interval} ç§’")

    def _rate_limit(self):
        """å®æ–½é€Ÿç‡é™åˆ¶"""
        current_time = time.time()
        time_since_last_request = current_time - self.last_request_time

        if time_since_last_request < self.min_request_interval:
            sleep_time = self.min_request_interval - time_since_last_request
            print(f"é€Ÿç‡é™åˆ¶ï¼šç­‰å¾… {sleep_time:.2f} ç§’...")
            time.sleep(sleep_time)

        self.last_request_time = time.time()

    def call_llm(self, prompt: str, max_tokens: int = None, temperature: float = None, max_retries: int = None) -> str:
        """è°ƒç”¨OpenRouter APIï¼Œå¸¦æœ‰é‡è¯•æœºåˆ¶å’Œé€Ÿç‡é™åˆ¶"""
        self._rate_limit()

        # ä½¿ç”¨é…ç½®çš„é»˜è®¤å€¼
        max_tokens = max_tokens or DEFAULT_MAX_TOKENS
        temperature = temperature or DEFAULT_TEMPERATURE
        max_retries = max_retries or MAX_RETRIES

        for attempt in range(max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=temperature,
                    max_tokens=max_tokens
                )

                return response.choices[0].message.content.strip()

            except Exception as e:
                if attempt == max_retries - 1:
                    raise Exception(f"LLM APIè°ƒç”¨å¤±è´¥: {str(e)}")
                else:
                    wait_time = (attempt + 1) * 2
                    print(f"ç¬¬{attempt + 1}æ¬¡APIè°ƒç”¨å¤±è´¥ï¼Œ{wait_time}ç§’åé‡è¯•: {str(e)}")
                    time.sleep(wait_time)
                    continue


def create_topic_hierarchy_prompt(title: str, primary_topic: str, text: str, keywords: str, strategy: str = "moderate") -> str:
    """åˆ›å»ºä¸»é¢˜å±‚æ¬¡åŒ–åˆ†æçš„æç¤ºè¯"""

    # æˆªå–æ–‡æœ¬é¢„è§ˆ
    text_preview = text[:TEXT_PREVIEW_LENGTH] if len(text) > TEXT_PREVIEW_LENGTH else text

    # æ ¹æ®ç­–ç•¥é€‰æ‹©æç¤ºè¯æ¨¡æ¿
    try:
        if strategy == "strict":
            template = PROMPT_TEMPLATE_STRICT
        elif strategy == "maximum":
            template = PROMPT_TEMPLATE_MAXIMUM
        elif strategy == "moderate":
            template = PROMPT_TEMPLATE_MODERATE
        else:  # focused (é»˜è®¤æ¨è)
            template = PROMPT_TEMPLATE_FOCUSED
    except NameError:
        # å¦‚æœæ²¡æœ‰å¯¼å…¥æ–°çš„æ¨¡æ¿ï¼Œä½¿ç”¨é»˜è®¤çš„
        template = PROMPT_TEMPLATE

    # ä½¿ç”¨é€‰æ‹©çš„æç¤ºè¯æ¨¡æ¿
    prompt = template.format(
        title=title,
        primary_topic=primary_topic,
        keywords=keywords,
        text_preview=text_preview
    )

    return prompt


def parse_topic_response(response: str) -> Dict[str, List[str]]:
    """è§£æLLMè¿”å›çš„ä¸»é¢˜å±‚æ¬¡ç»“æ„"""
    try:
        # å°è¯•ç›´æ¥è§£æJSON
        if response.strip().startswith('{'):
            result = json.loads(response.strip())
        else:
            # å¦‚æœå“åº”åŒ…å«ä»£ç å—ï¼Œæå–JSONéƒ¨åˆ†
            import re
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group(1))
            else:
                # å°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ª{åˆ°æœ€åä¸€ä¸ª}ä¹‹é—´çš„å†…å®¹
                start_idx = response.find('{')
                end_idx = response.rfind('}')
                if start_idx != -1 and end_idx != -1:
                    json_str = response[start_idx:end_idx+1]
                    result = json.loads(json_str)
                else:
                    raise ValueError("æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„JSONæ ¼å¼")

        # éªŒè¯ç»“æœæ ¼å¼
        if not isinstance(result, dict):
            raise ValueError("è¿”å›ç»“æœä¸æ˜¯å­—å…¸æ ¼å¼")

        secondary_topics = result.get("secondary_topics", [])
        tertiary_topics = result.get("tertiary_topics", [])

        if not isinstance(secondary_topics, list):
            secondary_topics = []
        if not isinstance(tertiary_topics, list):
            tertiary_topics = []

        return {
            "secondary_topics": secondary_topics,
            "tertiary_topics": tertiary_topics
        }

    except Exception as e:
        print(f"è§£æLLMå“åº”å¤±è´¥: {str(e)}")
        print(f"åŸå§‹å“åº”: {response[:200]}...")
        return {
            "secondary_topics": [],
            "tertiary_topics": []
        }


# æ¨èçš„å…è´¹æ¨¡å‹åˆ—è¡¨ï¼ˆå‚è€ƒtopic_evaluator.pyï¼‰
RECOMMENDED_MODELS = [
    "qwen/qwen3-14b:free",           # é»˜è®¤æ¨èï¼Œæ€§èƒ½å¥½
    "qwen/qwen3-coder:free",         # ä»£ç ç†è§£èƒ½åŠ›å¼º
    "meta-llama/llama-3.3-70b-instruct:free",
    "google/gemini-2.0-flash-exp:free",
    "deepseek/deepseek-r1-0528:free",
    "deepseek/deepseek-chat-v3.1:free"
]


def analyze_context_distribution(df: pd.DataFrame, min_text_length: int = 500) -> dict:
    """
    åˆ†ææ•°æ®çš„ä¸Šä¸‹æ–‡é•¿åº¦åˆ†å¸ƒï¼Œç¡®å®šåˆç†çš„é‡‡æ ·ç­–ç•¥

    Args:
        df: åŸå§‹æ•°æ®æ¡†
        min_text_length: æœ€å°æ–‡æœ¬é•¿åº¦

    Returns:
        åˆ†æç»“æœå­—å…¸
    """
    print(f"ğŸ“Š åˆ†ææ•°æ®ä¸Šä¸‹æ–‡é•¿åº¦åˆ†å¸ƒ...")

    # è®¡ç®—æ–‡æœ¬é•¿åº¦
    df['text_length'] = df['text'].astype(str).str.len()
    df_filtered = df[df['text_length'] >= min_text_length].copy()

    # è®¡ç®—ä¸Šä¸‹æ–‡æ€»é•¿åº¦
    def calc_context_length(row):
        title_len = len(str(row.get('title', '')))
        text_len = len(str(row.get('text', '')))
        topic_len = len(str(row.get('topic', '')))
        keywords_len = len(str(row.get('keywords', '')))
        return title_len + text_len + topic_len + keywords_len

    df_filtered['context_length'] = df_filtered.apply(calc_context_length, axis=1)

    # åˆ†æåˆ†å¸ƒ
    context_lengths = df_filtered['context_length']

    # è®¡ç®—åˆ†ä½æ•°
    percentiles = [50, 70, 80, 90, 95, 99]
    percentile_values = {}
    for p in percentiles:
        percentile_values[p] = context_lengths.quantile(p/100)

    # ç¡®å®šé‡‡æ ·ç­–ç•¥
    total_count = len(df_filtered)

    # é‡‡æ ·ç­–ç•¥ï¼šé€‰æ‹©å‰10-20%çš„æœ€å¤§ä¸Šä¸‹æ–‡æ•°æ®
    if total_count >= 1000:
        # å¤§æ•°æ®é›†ï¼šé€‰æ‹©å‰10%
        sample_ratio = 0.10
        strategy = "å¤§æ•°æ®é›†ç­–ç•¥"
    elif total_count >= 500:
        # ä¸­ç­‰æ•°æ®é›†ï¼šé€‰æ‹©å‰15%
        sample_ratio = 0.15
        strategy = "ä¸­ç­‰æ•°æ®é›†ç­–ç•¥"
    else:
        # å°æ•°æ®é›†ï¼šé€‰æ‹©å‰20%
        sample_ratio = 0.20
        strategy = "å°æ•°æ®é›†ç­–ç•¥"

    suggested_sample_size = max(50, int(total_count * sample_ratio))  # è‡³å°‘50æ¡
    suggested_sample_size = min(suggested_sample_size, 500)  # æœ€å¤š500æ¡

    analysis = {
        'total_count': total_count,
        'min_length': int(context_lengths.min()),
        'max_length': int(context_lengths.max()),
        'mean_length': int(context_lengths.mean()),
        'median_length': int(context_lengths.median()),
        'percentiles': percentile_values,
        'suggested_sample_size': suggested_sample_size,
        'sample_ratio': sample_ratio,
        'strategy': strategy
    }

    return analysis


def sample_max_context_data(df: pd.DataFrame, min_text_length: int = 500, auto_determine_size: bool = True, manual_sample_size: int = None) -> pd.DataFrame:
    """
    ä»æ•°æ®é›†ä¸­é‡‡æ ·æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦çš„æ•°æ®

    Args:
        df: åŸå§‹æ•°æ®æ¡†
        min_text_length: æœ€å°æ–‡æœ¬é•¿åº¦
        auto_determine_size: æ˜¯å¦è‡ªåŠ¨ç¡®å®šé‡‡æ ·æ•°é‡
        manual_sample_size: æ‰‹åŠ¨æŒ‡å®šçš„é‡‡æ ·æ•°é‡ï¼ˆå½“auto_determine_size=Falseæ—¶ä½¿ç”¨ï¼‰

    Returns:
        é‡‡æ ·åçš„æ•°æ®æ¡†
    """
    print(f"ğŸ“Š å¼€å§‹é‡‡æ ·æœ€å¤§ä¸Šä¸‹æ–‡æ•°æ®...")
    print(f"   åŸå§‹æ•°æ®é‡: {len(df):,}")

    # è®¡ç®—æ–‡æœ¬é•¿åº¦ï¼ˆä¸è¿‡æ»¤ï¼Œåªç”¨äºç»Ÿè®¡ï¼‰
    df['text_length'] = df['text'].astype(str).str.len()
    df_filtered = df.copy()  # ä¸è¿‡æ»¤ä»»ä½•æ•°æ®
    print(f"   æ•°æ®é‡: {len(df_filtered):,} (ä¸é™åˆ¶æ–‡æœ¬é•¿åº¦)")

    if len(df_filtered) == 0:
        print("âŒ æ²¡æœ‰æ•°æ®")
        return df.head(0)

    # è®¡ç®—æ¯è¡Œçš„ä¸Šä¸‹æ–‡æ€»é•¿åº¦
    def calc_context_length(row):
        title_len = len(str(row.get('title', '')))
        text_len = len(str(row.get('text', '')))
        topic_len = len(str(row.get('topic', '')))
        keywords_len = len(str(row.get('keywords', '')))
        return title_len + text_len + topic_len + keywords_len

    print("ğŸ“ è®¡ç®—ä¸Šä¸‹æ–‡æ€»é•¿åº¦...")
    df_filtered['context_length'] = df_filtered.apply(calc_context_length, axis=1)

    # ç¡®å®šé‡‡æ ·æ•°é‡
    if auto_determine_size:
        analysis = analyze_context_distribution(df_filtered, min_text_length)
        sample_size = analysis['suggested_sample_size']

        print(f"\nğŸ“ˆ æ•°æ®åˆ†æç»“æœ:")
        print(f"   é‡‡æ ·ç­–ç•¥: {analysis['strategy']}")
        print(f"   å»ºè®®é‡‡æ ·æ•°é‡: {sample_size} æ¡ ({analysis['sample_ratio']*100:.0f}%)")
        print(f"   ä¸Šä¸‹æ–‡é•¿åº¦åˆ†å¸ƒ:")
        print(f"     æœ€å°: {analysis['min_length']:,}")
        print(f"     ä¸­ä½æ•°: {analysis['median_length']:,}")
        print(f"     å¹³å‡: {analysis['mean_length']:,}")
        print(f"     æœ€å¤§: {analysis['max_length']:,}")
        print(f"   åˆ†ä½æ•°åˆ†æ:")
        for p, v in analysis['percentiles'].items():
            print(f"     {p}%åˆ†ä½æ•°: {v:,.0f}")
    else:
        sample_size = manual_sample_size or 100
        print(f"   æ‰‹åŠ¨æŒ‡å®šé‡‡æ ·æ•°é‡: {sample_size}")

    # æŒ‰ä¸Šä¸‹æ–‡é•¿åº¦æ’åºï¼Œå–æœ€å¤§çš„
    df_sorted = df_filtered.sort_values('context_length', ascending=False)
    actual_sample_size = min(sample_size, len(df_sorted))
    df_sample = df_sorted.head(actual_sample_size).copy()

    # ç»Ÿè®¡ä¿¡æ¯
    print(f"\nï¿½ æœ€ç»ˆé‡‡æ ·ç»Ÿè®¡:")
    print(f"   å®é™…é‡‡æ ·æ•°é‡: {len(df_sample)}")
    print(f"   é‡‡æ ·æ¯”ä¾‹: {len(df_sample)/len(df_filtered)*100:.1f}%")
    print(f"   æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦: {df_sample['context_length'].max():,}")
    print(f"   æœ€å°ä¸Šä¸‹æ–‡é•¿åº¦: {df_sample['context_length'].min():,}")
    print(f"   å¹³å‡ä¸Šä¸‹æ–‡é•¿åº¦: {df_sample['context_length'].mean():.0f}")
    print(f"   æœ€å¤§æ–‡æœ¬é•¿åº¦: {df_sample['text_length'].max():,}")
    print(f"   å¹³å‡æ–‡æœ¬é•¿åº¦: {df_sample['text_length'].mean():.0f}")

    # ä¸»é¢˜åˆ†å¸ƒ
    if 'topic' in df_sample.columns:
        topic_counts = df_sample['topic'].value_counts()
        print(f"\nğŸ“‹ é‡‡æ ·æ•°æ®ä¸»é¢˜åˆ†å¸ƒ:")
        for topic, count in topic_counts.head(8).items():
            print(f"   {topic}: {count} æ¡")

    # ç§»é™¤è¾…åŠ©åˆ—å¹¶é‡ç½®ç´¢å¼•
    df_result = df_sample.drop(['text_length', 'context_length'], axis=1)
    df_result = df_result.reset_index(drop=True)  # é‡ç½®ç´¢å¼•ï¼Œç¡®ä¿ä»0å¼€å§‹è¿ç»­
    return df_result


def process_nyt_dataset(
    input_file: str,
    api_key: str = None,
    model: str = None,  # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤æ¨¡å‹
    strategy: str = "moderate",  # ä¸»é¢˜ç”Ÿæˆç­–ç•¥
    sample_size: int = 1000,  # é‡‡æ ·æ•°é‡
    min_text_length: int = 500,  # æœ€å°æ–‡æœ¬é•¿åº¦
    start_row: int = 0,
    max_rows: int = None,
    save_interval: int = 10
) -> None:
    """
    å¤„ç†NYTæ•°æ®é›†ï¼Œé‡‡æ ·æœ€å¤§ä¸Šä¸‹æ–‡æ•°æ®ï¼Œä¸ºæ¯è¡Œç”ŸæˆäºŒ/ä¸‰çº§ä¸»é¢˜

    Args:
        input_file: è¾“å…¥CSVæ–‡ä»¶è·¯å¾„
        api_key: OpenRouter APIå¯†é’¥
        model: ä½¿ç”¨çš„LLMæ¨¡å‹
        strategy: ä¸»é¢˜ç”Ÿæˆç­–ç•¥ ("focused", "moderate", "strict", "maximum")
        sample_size: é‡‡æ ·æ•°é‡ï¼ˆé€‰æ‹©å‰Næ¡æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦çš„æ•°æ®ï¼‰
        min_text_length: æœ€å°æ–‡æœ¬é•¿åº¦è¦æ±‚
        start_row: å¼€å§‹å¤„ç†çš„è¡Œå·ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰
        max_rows: æœ€å¤§å¤„ç†è¡Œæ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        save_interval: æ¯å¤„ç†å¤šå°‘è¡Œä¿å­˜ä¸€æ¬¡

    é‡‡æ ·ç­–ç•¥ï¼š
        - é€‰æ‹©å‰sample_sizeæ¡æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦çš„æ•°æ®
        - ä¸é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œç¡®ä¿èƒ½é‡‡æ ·åˆ°è¶³å¤Ÿæ•°é‡çš„æ•°æ®
    """

    # ä½¿ç”¨é»˜è®¤æ¨¡å‹å¦‚æœæœªæŒ‡å®š
    model = model or DEFAULT_MODEL

    print(f"=== NYTæ•°æ®é›†ä¸»é¢˜å±‚æ¬¡åŒ–å¤„ç†å¼€å§‹ ===")
    print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {input_file}")
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model}")
    print(f"ğŸ¯ ä¸»é¢˜ç”Ÿæˆç­–ç•¥: {strategy}")
    print(f"ğŸ“Š é‡‡æ ·è®¾ç½®: {sample_size}æ¡æœ€å¤§ä¸Šä¸‹æ–‡æ•°æ® (ä¸é™åˆ¶æ–‡æœ¬é•¿åº¦)")
    print(f"ğŸ’¾ ä¿å­˜é—´éš”: æ¯{save_interval}è¡Œ")

    # è¾“å‡ºæ–‡ä»¶åï¼ˆåŸºäºé‡‡æ ·ï¼‰
    base_name = os.path.splitext(os.path.basename(input_file))[0]
    output_file = f"{base_name}_sampled_with_topics.csv"
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(input_file):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return

    # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
    try:
        llm_client = OpenRouterClient(api_key=api_key, model=model)
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–LLMå®¢æˆ·ç«¯å¤±è´¥: {str(e)}")
        return

    # è¯»å–CSVæ–‡ä»¶
    try:
        print(f"ğŸ“– æ­£åœ¨è¯»å–CSVæ–‡ä»¶...")
        # å°è¯•ä¸åŒçš„ç¼–ç æ ¼å¼ï¼ˆä½¿ç”¨é…ç½®ï¼‰
        encodings = DEFAULT_ENCODING_OPTIONS
        df = None
        for encoding in encodings:
            try:
                df = pd.read_csv(input_file, encoding=encoding)
                print(f"âœ… æˆåŠŸè¯»å– {len(df)} è¡Œæ•°æ® (ç¼–ç : {encoding})")
                break
            except UnicodeDecodeError:
                continue

        if df is None:
            raise ValueError("æ— æ³•ä½¿ç”¨å¸¸è§ç¼–ç æ ¼å¼è¯»å–CSVæ–‡ä»¶")

        # è¿›è¡Œæœ€å¤§ä¸Šä¸‹æ–‡é‡‡æ ·ï¼ˆå›ºå®š1000æ¡ï¼‰
        print(f"\n" + "="*50)
        df_sampled = sample_max_context_data(df, min_text_length=min_text_length, auto_determine_size=False, manual_sample_size=sample_size)
        if len(df_sampled) == 0:
            print("âŒ é‡‡æ ·å¤±è´¥ï¼Œæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ•°æ®")
            return

        print(f"âœ… é‡‡æ ·å®Œæˆï¼Œå°†å¤„ç† {len(df_sampled)} æ¡æœ€å¤§ä¸Šä¸‹æ–‡æ•°æ®")
        df = df_sampled  # ä½¿ç”¨é‡‡æ ·åçš„æ•°æ®

        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        required_columns = ['title', 'topic', 'text', 'keywords']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}")

    except Exception as e:
        print(f"âŒ è¯»å–CSVæ–‡ä»¶å¤±è´¥: {str(e)}")
        return

    # é‡ç½®ç´¢å¼•ï¼Œç¡®ä¿ç´¢å¼•è¿ç»­
    df = df.reset_index(drop=True)

    # æ·»åŠ ä¸»é¢˜åˆ—ï¼ˆç´§æ¥ç€keywordsåˆ—åé¢ï¼‰
    if 'secondary_topics' not in df.columns:
        # æ‰¾åˆ°keywordsåˆ—çš„ä½ç½®
        keywords_pos = df.columns.get_loc('keywords')
        # åœ¨keywordsåé¢æ’å…¥æ–°åˆ—
        df.insert(keywords_pos + 1, 'secondary_topics', '')
    if 'tertiary_topics' not in df.columns:
        # æ‰¾åˆ°secondary_topicsåˆ—çš„ä½ç½®
        secondary_pos = df.columns.get_loc('secondary_topics')
        # åœ¨secondary_topicsåé¢æ’å…¥tertiary_topics
        df.insert(secondary_pos + 1, 'tertiary_topics', '')

    # å¦‚æœmax_rows=0ï¼Œåªä¿å­˜é‡‡æ ·æ•°æ®ï¼Œä¸è¿›è¡Œä¸»é¢˜ç”Ÿæˆ
    if max_rows == 0:
        print("ğŸ“Š åªé‡‡æ ·æ¨¡å¼ï¼šä¿å­˜é‡‡æ ·æ•°æ®ï¼Œä¸ç”Ÿæˆä¸»é¢˜")

        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        base_name = os.path.splitext(os.path.basename(input_file))[0]
        output_file = f"{base_name}_sampled.csv"
        output_path = os.path.join(os.path.dirname(input_file), output_file)

        # ä¿å­˜é‡‡æ ·æ•°æ®
        df.to_csv(output_path, index=False, encoding='utf-8')
        print(f"âœ… é‡‡æ ·æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
        print(f"ğŸ“Š é‡‡æ ·è¡Œæ•°: {len(df):,}")
        return

    # ç¡®å®šå¤„ç†èŒƒå›´
    total_rows = len(df)
    end_row = min(start_row + max_rows, total_rows) if max_rows else total_rows

    print(f"ğŸ“Š å¤„ç†èŒƒå›´: ç¬¬{start_row}è¡Œ åˆ° ç¬¬{end_row-1}è¡Œ (å…±{end_row-start_row}è¡Œ)")

    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤´éƒ¨ï¼ˆå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼‰
    if not os.path.exists(output_file):
        print(f"ğŸ“ åˆ›å»ºè¾“å‡ºæ–‡ä»¶: {output_file}")
        df.head(0).to_csv(output_file, index=False, encoding='utf-8')  # åªå†™å…¥åˆ—å¤´

    # ç»Ÿè®¡ä¿¡æ¯
    processed_count = 0
    success_count = 0
    error_count = 0
    start_time = time.time()

    try:
        for idx in range(start_row, end_row):
            row = df.iloc[idx]

            print(f"\nğŸ”„ å¤„ç†ç¬¬{idx}è¡Œ (è¿›åº¦: {idx-start_row+1}/{end_row-start_row})")
            print(f"   æ ‡é¢˜: {row['title'][:50]}...")
            print(f"   åŸå§‹ç´¢å¼•: {idx}")  # æ˜¾ç¤ºå½“å‰å¤„ç†çš„ç´¢å¼•

            # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡
            if pd.notna(row['secondary_topics']) and row['secondary_topics'].strip():
                print(f"   â­ï¸  å·²å¤„ç†è¿‡ï¼Œè·³è¿‡")
                processed_count += 1
                continue

            try:
                # åˆ›å»ºæç¤ºè¯
                prompt = create_topic_hierarchy_prompt(
                    title=str(row['title']),
                    primary_topic=str(row['topic']),
                    text=str(row['text']),
                    keywords=str(row['keywords']),
                    strategy=strategy
                )

                # è°ƒç”¨LLM
                print(f"   ğŸ¤– æ­£åœ¨è°ƒç”¨LLM...")
                llm_response = llm_client.call_llm(prompt, max_tokens=2000, temperature=0.3)

                # æ·»åŠ è¯·æ±‚é—´éš”ï¼Œé¿å…è§¦å‘é€Ÿç‡é™åˆ¶
                print(f"   â±ï¸  ç­‰å¾…3ç§’é¿å…é€Ÿç‡é™åˆ¶...")
                time.sleep(3)

                # è§£æå“åº”
                topic_hierarchy = parse_topic_response(llm_response)

                # å°†ä¸»é¢˜åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼ˆä½¿ç”¨é…ç½®çš„åˆ†éš”ç¬¦ï¼‰
                secondary_topics_str = TOPIC_SEPARATOR.join(topic_hierarchy['secondary_topics'])
                tertiary_topics_str = TOPIC_SEPARATOR.join(topic_hierarchy['tertiary_topics'])

                # æ›´æ–°DataFrameï¼ˆä½¿ç”¨ilocç¡®ä¿æ­£ç¡®çš„è¡Œç´¢å¼•ï¼‰
                df.iloc[idx, df.columns.get_loc('secondary_topics')] = secondary_topics_str
                df.iloc[idx, df.columns.get_loc('tertiary_topics')] = tertiary_topics_str

                print(f"   âœ… æˆåŠŸç”Ÿæˆä¸»é¢˜:")
                print(f"      äºŒçº§ä¸»é¢˜({len(topic_hierarchy['secondary_topics'])}ä¸ª): {secondary_topics_str[:100]}...")
                print(f"      ä¸‰çº§ä¸»é¢˜({len(topic_hierarchy['tertiary_topics'])}ä¸ª): {tertiary_topics_str[:100]}...")

                # ç«‹å³ä¿å­˜è¿™ä¸€è¡Œåˆ°æ–‡ä»¶
                row_to_save = df.iloc[idx:idx+1]  # è·å–å½“å‰è¡Œ
                if idx == start_row:
                    # ç¬¬ä¸€è¡Œï¼šè¦†ç›–æ–‡ä»¶ï¼ˆåŒ…å«åˆ—å¤´ï¼‰
                    row_to_save.to_csv(output_file, index=False, encoding='utf-8', mode='w')
                    print(f"   ğŸ’¾ å·²ä¿å­˜ç¬¬{idx}è¡Œåˆ°æ–‡ä»¶ï¼ˆåˆ›å»ºæ–°æ–‡ä»¶ï¼‰")
                else:
                    # åç»­è¡Œï¼šè¿½åŠ åˆ°æ–‡ä»¶ï¼ˆä¸åŒ…å«åˆ—å¤´ï¼‰
                    row_to_save.to_csv(output_file, index=False, encoding='utf-8', mode='a', header=False)
                    print(f"   ğŸ’¾ å·²ä¿å­˜ç¬¬{idx}è¡Œåˆ°æ–‡ä»¶")

                success_count += 1

            except Exception as e:
                print(f"   âŒ å¤„ç†å¤±è´¥: {str(e)}")
                df.iloc[idx, df.columns.get_loc('secondary_topics')] = ERROR_MARKER
                df.iloc[idx, df.columns.get_loc('tertiary_topics')] = ERROR_MARKER

                # ç«‹å³ä¿å­˜é”™è¯¯æ ‡è®°çš„è¡Œ
                row_to_save = df.iloc[idx:idx+1]
                if idx == start_row:
                    row_to_save.to_csv(output_file, index=False, encoding='utf-8', mode='w')
                    print(f"   ğŸ’¾ å·²ä¿å­˜ç¬¬{idx}è¡Œåˆ°æ–‡ä»¶ï¼ˆé”™è¯¯æ ‡è®°ï¼‰")
                else:
                    row_to_save.to_csv(output_file, index=False, encoding='utf-8', mode='a', header=False)
                    print(f"   ğŸ’¾ å·²ä¿å­˜ç¬¬{idx}è¡Œåˆ°æ–‡ä»¶ï¼ˆé”™è¯¯æ ‡è®°ï¼‰")

                error_count += 1

            processed_count += 1

            # æ˜¾ç¤ºè¿›åº¦ç»Ÿè®¡ï¼ˆæ¯10è¡Œæ˜¾ç¤ºä¸€æ¬¡ï¼‰
            if processed_count % 10 == 0:
                elapsed_time = time.time() - start_time
                avg_time_per_row = elapsed_time / processed_count
                remaining_rows = end_row - idx - 1
                estimated_remaining_time = remaining_rows * avg_time_per_row

                print(f"\nğŸ“Š è¿›åº¦ç»Ÿè®¡:")
                print(f"   å·²å¤„ç†: {processed_count}/{end_row-start_row} è¡Œ")
                print(f"   æˆåŠŸ: {success_count}, å¤±è´¥: {error_count}")
                print(f"   å¹³å‡è€—æ—¶: {avg_time_per_row:.1f}ç§’/è¡Œ")
                print(f"   é¢„è®¡å‰©ä½™æ—¶é—´: {estimated_remaining_time/60:.1f}åˆ†é’Ÿ")
                print(f"   è¾“å‡ºæ–‡ä»¶: {output_file}")

    except KeyboardInterrupt:
        print(f"\nâš ï¸  ç”¨æˆ·ä¸­æ–­å¤„ç†ï¼Œæ­£åœ¨ä¿å­˜å½“å‰è¿›åº¦...")

    except Exception as e:
        print(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")

    finally:
        # æœ€ç»ˆç»Ÿè®¡ï¼ˆä¸éœ€è¦å†æ¬¡ä¿å­˜ï¼Œå› ä¸ºå·²ç»å®æ—¶ä¿å­˜äº†ï¼‰
        total_time = time.time() - start_time
        print(f"\n=== å¤„ç†å®Œæˆ ===")
        print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        print(f"   æ€»å¤„ç†è¡Œæ•°: {processed_count}")
        print(f"   æˆåŠŸ: {success_count}")
        print(f"   å¤±è´¥: {error_count}")
        print(f"   æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
        if processed_count > 0:
            print(f"   å¹³å‡è€—æ—¶: {total_time/processed_count:.2f} ç§’/è¡Œ" if processed_count > 0 else "")
        print(f"   è¾“å‡ºæ–‡ä»¶: {output_file}")
        print(f"ğŸ’¡ æ‰€æœ‰æ•°æ®å·²å®æ—¶ä¿å­˜ï¼Œæ— éœ€ç­‰å¾…æœ€ç»ˆä¿å­˜")
        print(f"   åŸå§‹æ–‡ä»¶: {input_file} (æœªä¿®æ”¹)")


def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¨¡å¼"""
    import argparse

    parser = argparse.ArgumentParser(description='NYTæ•°æ®é›†ä¸»é¢˜å±‚æ¬¡åŒ–å¤„ç†ï¼ˆç›´æ¥åœ¨åŸæ–‡ä»¶ä¸Šä¿®æ”¹ï¼‰')
    parser.add_argument('--input', '-i', required=True, help='è¾“å…¥CSVæ–‡ä»¶è·¯å¾„ï¼ˆå°†ç›´æ¥åœ¨æ­¤æ–‡ä»¶ä¸Šæ·»åŠ æ–°åˆ—ï¼‰')
    parser.add_argument('--api-key', help='OpenRouter APIå¯†é’¥')
    parser.add_argument('--model', default=DEFAULT_MODEL, help='LLMæ¨¡å‹åç§°')
    parser.add_argument('--start-row', type=int, default=0, help='å¼€å§‹å¤„ç†çš„è¡Œå·')
    parser.add_argument('--max-rows', type=int, help='æœ€å¤§å¤„ç†è¡Œæ•°')
    parser.add_argument('--save-interval', type=int, default=10, help='ä¿å­˜é—´éš”')

    args = parser.parse_args()

    process_nyt_dataset(
        input_file=args.input,
        api_key=args.api_key,
        model=args.model,
        start_row=args.start_row,
        max_rows=args.max_rows,
        save_interval=args.save_interval
    )


if __name__ == "__main__":
    # ==================== é…ç½®åŒºåŸŸ ====================
    # åœ¨è¿™é‡Œä¿®æ”¹æ‚¨çš„è®¾ç½®

    # APIå¯†é’¥ï¼ˆå‚è€ƒtopic_evaluator.pyçš„æ–¹å¼ï¼‰
    API_KEY = "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"  # å®é™…APIå¯†é’¥

    # æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆä¿®æ”¹ä¸ºæ‚¨çš„æ–‡ä»¶è·¯å¾„ï¼‰
    input_file = "../../data/NYT_Dataset.csv"

    # ä½¿ç”¨çš„æ¨¡å‹ï¼ˆä»ä¸‹é¢é€‰æ‹©ä¸€ä¸ªï¼Œå–æ¶ˆæ³¨é‡Šå³å¯ï¼‰
    selected_model = DEFAULT_MODEL  # ä½¿ç”¨é»˜è®¤æ¨¡å‹: qwen/qwen3-14b:freeï¼ˆæ›´ç¨³å®šï¼‰
    # selected_model = "qwen/qwen3-coder:free"                  # ä»£ç ç†è§£èƒ½åŠ›å¼ºï¼ˆä½†é™æµä¸¥é‡ï¼‰
    # selected_model = "meta-llama/llama-3.3-70b-instruct:free" # å¤§æ¨¡å‹
    # selected_model = "google/gemini-2.0-flash-exp:free"       # Googleæ¨¡å‹
    # selected_model = "deepseek/deepseek-r1-0528:free"         # DeepSeekæ¨¡å‹

    # ä¸»é¢˜ç”Ÿæˆç­–ç•¥é€‰æ‹©ï¼ˆé‡è¦ï¼æ§åˆ¶ç”Ÿæˆä¸»é¢˜æ•°é‡ï¼‰
    # "focused"  = èšç„¦æ ¸å¿ƒï¼ˆ2-4ä¸ªäºŒçº§ä¸»é¢˜ï¼Œ3-6ä¸ªä¸‰çº§ä¸»é¢˜ï¼‰ã€æ¨èã€‘
    # "moderate" = é€‚åº¦æ§åˆ¶ï¼ˆ2-5ä¸ªäºŒçº§ä¸»é¢˜ï¼Œ3-8ä¸ªä¸‰çº§ä¸»é¢˜ï¼‰
    # "strict"   = ä¸¥æ ¼æ§åˆ¶ï¼ˆ2-3ä¸ªäºŒçº§ä¸»é¢˜ï¼Œ3-5ä¸ªä¸‰çº§ä¸»é¢˜ï¼‰
    # "maximum"  = æœ€å¤§ç”Ÿæˆï¼ˆå°½å¯èƒ½å¤šçš„ä¸»é¢˜ï¼‰
    topic_generation_strategy = "focused"

    # é‡‡æ ·è®¾ç½®ï¼ˆä»åŸå§‹æ•°æ®ä¸­é€‰æ‹©æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦çš„æ•°æ®ï¼‰
    sample_size = 14000       # é‡‡æ ·æ•°é‡ï¼šé€‰æ‹©å‰14000æ¡æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦çš„æ•°æ®
    min_text_length = 0       # ä¸é™åˆ¶æœ€å°æ–‡æœ¬é•¿åº¦ï¼Œä¿è¯èƒ½é‡‡æ ·åˆ°14000æ¡

    # å¤„ç†è®¾ç½®
    process_all_data = False   # True=å¤„ç†å…¨éƒ¨æ•°æ®, False=åªå¤„ç†éƒ¨åˆ†æ•°æ®
    test_rows = 0             # å¦‚æœprocess_all_data=Falseï¼Œå¤„ç†å¤šå°‘è¡Œï¼ˆ0=åªé‡‡æ ·ä¸ç”Ÿæˆä¸»é¢˜ï¼‰
    save_every = 50           # æ¯å¤„ç†å¤šå°‘è¡Œä¿å­˜ä¸€æ¬¡

    # ==================== é…ç½®åŒºåŸŸç»“æŸ ====================

    print("ğŸš€ å¼€å§‹å¤„ç†NYTæ•°æ®é›†...")
    print("ğŸ’¡ æç¤ºï¼šä½¿ç”¨è„šæœ¬å†…è®¾ç½®çš„APIå¯†é’¥")
    print(f"ï¿½ ä½¿ç”¨æ¨¡å‹: {selected_model}")
    print(f"ğŸ“‹ æ¨èçš„å…è´¹æ¨¡å‹: {', '.join(RECOMMENDED_MODELS[:3])}")
    print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {input_file}")
    print(f"ğŸ“Š é‡‡æ ·è®¾ç½®: {sample_size}æ¡æœ€å¤§ä¸Šä¸‹æ–‡æ•°æ® (ä¸é™åˆ¶æ–‡æœ¬é•¿åº¦)")
    print(f"âš™ï¸  å¤„ç†æ¨¡å¼: {'å…¨éƒ¨é‡‡æ ·æ•°æ®' if process_all_data else f'æµ‹è¯•æ¨¡å¼({test_rows}è¡Œ)'}")

    # æ˜¾ç¤ºä¸»é¢˜ç”Ÿæˆç­–ç•¥
    strategy_descriptions = {
        "focused": "èšç„¦æ ¸å¿ƒï¼ˆ2-4ä¸ªäºŒçº§ä¸»é¢˜ï¼Œ3-6ä¸ªä¸‰çº§ä¸»é¢˜ï¼‰",
        "moderate": "é€‚åº¦æ§åˆ¶ï¼ˆ2-5ä¸ªäºŒçº§ä¸»é¢˜ï¼Œ3-8ä¸ªä¸‰çº§ä¸»é¢˜ï¼‰",
        "strict": "ä¸¥æ ¼æ§åˆ¶ï¼ˆ2-3ä¸ªäºŒçº§ä¸»é¢˜ï¼Œ3-5ä¸ªä¸‰çº§ä¸»é¢˜ï¼‰",
        "maximum": "æœ€å¤§ç”Ÿæˆï¼ˆå°½å¯èƒ½å¤šçš„ä¸»é¢˜ï¼‰"
    }
    print(f"ğŸ¯ ä¸»é¢˜ç”Ÿæˆç­–ç•¥: {topic_generation_strategy} - {strategy_descriptions.get(topic_generation_strategy, 'æœªçŸ¥ç­–ç•¥')}")

    print(f"ğŸ”‘ APIå¯†é’¥: {API_KEY[:20]}...")  # åªæ˜¾ç¤ºå‰20ä¸ªå­—ç¬¦

    # æ£€æŸ¥APIå¯†é’¥
    if API_KEY and API_KEY != "your_openrouter_api_key_here":
        api_key = API_KEY
        print("âœ… ä½¿ç”¨è„šæœ¬ä¸­è®¾ç½®çš„APIå¯†é’¥")
    else:
        print("\nâŒ é”™è¯¯ï¼šè¯·è®¾ç½®æ‚¨çš„APIå¯†é’¥")
        print("è¯·ä¿®æ”¹è„šæœ¬ä¸­çš„é…ç½®ï¼š")
        print('API_KEY = "your_actual_api_key_here"  # æ›¿æ¢ä¸ºæ‚¨çš„å®é™…APIå¯†é’¥')
        print("\nå¦‚ä½•è·å–APIå¯†é’¥ï¼š")
        print("1. è®¿é—® https://openrouter.ai/")
        print("2. æ³¨å†Œå¹¶ç™»å½•è´¦å·")
        print("3. åœ¨æ§åˆ¶å°è·å–APIå¯†é’¥")
        print("4. å°†å¯†é’¥å¡«å…¥ä¸Šé¢çš„API_KEYå˜é‡ä¸­")
        exit(1)

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(input_file):
        print(f"\nâŒ é”™è¯¯ï¼šè¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        exit(1)

    if process_all_data:
        # è¯¢é—®ç”¨æˆ·æ˜¯å¦è¦å¤„ç†å…¨éƒ¨é‡‡æ ·æ•°æ®
        print(f"\nâš ï¸  å³å°†ä»åŸå§‹æ•°æ®é›†ä¸­é‡‡æ · {sample_size} æ¡æœ€å¤§ä¸Šä¸‹æ–‡æ•°æ®å¹¶ç”Ÿæˆä¸»é¢˜")
        print("å¦‚æœåªæƒ³æµ‹è¯•ï¼Œè¯·ä¿®æ”¹è„šæœ¬ä¸­çš„ process_all_data = False")
        response = input("ç¡®è®¤å¼€å§‹é‡‡æ ·å’Œå¤„ç†ï¼Ÿ(y/n): ").lower().strip()

        if response != 'y':
            print("âŒ ç”¨æˆ·å–æ¶ˆå¤„ç†")
            exit(0)

        max_rows_to_process = None
        print("ğŸš€ å¼€å§‹é‡‡æ ·å’Œå¤„ç†...")
    else:
        max_rows_to_process = test_rows
        print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šé‡‡æ ·ååªå¤„ç†å‰ {test_rows} è¡Œ")

    try:
        # å¼€å§‹å¤„ç†
        process_nyt_dataset(
            input_file=input_file,
            api_key=api_key,
            model=selected_model,
            strategy=topic_generation_strategy,
            sample_size=sample_size,
            min_text_length=min_text_length,
            start_row=0,
            max_rows=max_rows_to_process,
            save_interval=save_every
        )

        if process_all_data:
            print("\nğŸ‰ é‡‡æ ·å’Œä¸»é¢˜ç”Ÿæˆå®Œæˆï¼")
        else:
            print(f"\nğŸ‰ æµ‹è¯•å¤„ç†å®Œæˆï¼å¤„ç†äº† {test_rows} è¡Œé‡‡æ ·æ•°æ®")

    except Exception as e:
        print(f"\nâŒ å¤„ç†å¤±è´¥: {str(e)}")
        exit(1)